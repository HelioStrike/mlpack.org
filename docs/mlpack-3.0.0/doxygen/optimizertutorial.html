<html >
<head >

<meta name="keywords" content="mlpack, libmlpack, c++, armadillo, machine
learning, data mining, classification, regression, tree-based methods, dual-tree
algorithm">
<meta name="description" content="mlpack: a scalable c++ machine learning
library">
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title >mlpack: a scalable c++ machine learning library</title>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript" src="dynamic_tables.js"></script>

<script src="../../../js/optimizer-visualization.js"></script>
<script src="../../../js/functions.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/vis/4.21.0/vis.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.7.1/Chart.min.js"></script>
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/vis/4.21.0/vis.min.css"/>
<link rel="stylesheet" type="text/css" href="../../../css/optimizer-visualization-style.css"/>


</head><link rel="stylesheet" href="style-doxygen.css" /><link rel="stylesheet" href="doxygen.css" /><link rel="stylesheet" href="tabs.css" /><link rel="stylesheet" href="search/search.css" /><link href="http://fonts.googleapis.com/css?family=Maven+Pro:500" rel="stylesheet" type="text/css" />





<body ><br />


<div class="mlpack_titlebar">
   <a href="http://www.mlpack.org"><img src="../../../mlpack.png"></a>
</div>
<center >
<div class="mlnavbar">
  <div class="navcontainer">
   <div class="mlnavitem" name="mlnavmain"><a href="../../../index.html">main</a></div>
   <div class="mlnavitem" name="mlnavabout"><a href="../../../about.html">about</a></div>
   <div class="mlnavitem" name="mlnavdoc"><a href="../../../docs.html">docs</a></div>
   <div class="mlnavitem" name="mlnavhelp"><a href="../../../help.html">get help</a></div>
   <div class="mlnavitem" name="mlnavbugs"><a href="https://github.com/mlpack/mlpack">github</a></div>
  </div>
</div>
</center>
<div class="separator"></div>
<center >
<div class="mainsection smallertext">
<div id="top">
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody >
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">mlpack
   &#160;<span id="projectnumber">3.0.0</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>

<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>

<div id="MSearchSelectWindow" onmouseover="return searchBox.OnSearchSelectShow()" onmouseout="return searchBox.OnSearchSelectHide()" onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>


<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div>
<div class="header">
  <div class="headertitle">
<div class="title">Optimizer implementation tutorial </div>  </div>
</div>
<div class="contents">
<div class="textblock"><h1 ><a class="anchor" id="intro_optimizertut"></a>
Introduction</h1><div align="right"><span class="mlabel" id="author" style="border: 0px solid #333333;">
    <a href="https://kurg.org" style="color:#ffffff !important">Author: Marcus Edel</a>
    </span></div><p >The field of optimization is vast and complex. In optimization problems, we have to find solutions which are optimal or near-optimal with respect to some goals. Usually, we are not able to solve problems in one step, but we follow some process which guides us through problem-solving. <code >mlpack</code> implements multiple strategies for optimizing different objective functions and provides different strategies for selecting and customizing the appropriate optimization algorithm. This tutorial discusses how to use each of the techniques that <code >mlpack</code> implements.</p>

<p>This visualization allows us to see how many popular optimizers perform on different optimization problems. Select a problem to optimize, then select an optimizer and tune its parameters, and see the steps that the optimizer takes plotted in pink. Note that you can zoom in and rotate the drawing.  Also, you can compare how different optimizers perform on a given problem in the second graph.  As you try a given problem with more optimizers, the objective function vs. the number of iterations is plotted for each optimizer you have tried.</p>

<div style="position: relative; width:600px; margin:0 auto;"> <div id="function" style="position: absolute; left: 50px; top: -150px;"></div><div id="optimizer" style="position: absolute; left: 50px; top: -150px;"></div></div><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><div id="functionWrapper" class="wrapper" style="width:610px; margin:0 auto;"> <div class="box a"><img id="f0" src="../../../img/fig_0.jpg" width="60px" style="border-radius: 5px; border: 3px solid #CC9900;" onclick="functionSelection(this)" onmouseover="functionOver(this)" onmouseout="functionOut(this)"></div><div class="box a"><img id="f8" src="../../../img/fig_8.jpg" width="60px" style="border-radius: 5px; border: 3px solid #333333;" onclick="functionSelection(this)" onmouseover="functionOver(this)" onmouseout="functionOut(this)"></div><div class="box a"><img id="f7" src="../../../img/fig_7.jpg" width="60px" style="border-radius: 5px; border: 3px solid #333333;" onclick="functionSelection(this)" onmouseover="functionOver(this)" onmouseout="functionOut(this)"></div><div class="box a"><img id="f5" src="../../../img/fig_5.jpg" width="60px" style="border-radius: 5px; border: 3px solid #333333;" onclick="functionSelection(this)" onmouseover="functionOver(this)" onmouseout="functionOut(this)"></div><div class="box a"><img id="f3" src="../../../img/fig_3.jpg" width="60px" style="border-radius: 5px; border: 3px solid #333333;" onclick="functionSelection(this)" onmouseover="functionOver(this)" onmouseout="functionOut(this)"></div><div class="box a"><img id="f1" src="../../../img/fig_1.jpg" width="60px" style="border-radius: 5px; border: 3px solid #333333;" onclick="functionSelection(this)" onmouseover="functionOver(this)" onmouseout="functionOut(this)"></div><div class="box a"><img id="f2" src="../../../img/fig_2.jpg" width="60px" style="border-radius: 5px; border: 3px solid #333333;" onclick="functionSelection(this)" onmouseover="functionOver(this)" onmouseout="functionOut(this)"></div><div class="box a"><img id="f4" src="../../../img/fig_4.jpg" width="60px" style="border-radius: 5px; border: 3px solid #333333;" onclick="functionSelection(this)" onmouseover="functionOver(this)" onmouseout="functionOut(this)"></div><div class="box a"><img id="f6" src="../../../img/fig_6.jpg" width="60px" style="border-radius: 5px; border: 3px solid #333333;" onclick="functionSelection(this)" onmouseover="functionOver(this)" onmouseout="functionOut(this)"></div></div><div class="wrapperA" style="width:600px; margin:0 auto;"> <div class="a"> <br><input id="stepsizeSlider" value="0.01" type="range" min="0" max="1" step="0.0001" onchange="stepsizeSliderChanged(this.value)"/> <p><code style="font-size: 0.8rem;"><span id="stepSizeDesc">Step-size:</span> <span id="stepsizeValue">0.01</span></code></p></div><div class="a"> <br><input id="iterationsSlider" value="4000" type="range" min="1" max="10000" step="1" onchange="iterationsSliderChanged(this.value)"/> <p><code style="font-size: 0.8rem;">Iterations: <span id="iterationsValue">4000</span></code></p></div><div class="a" style="left: 0px; top: 200px;"> <p class="small" style="color:#F1F1F1; font-size: 0.8rem;"> The defaults here are not necessarily good for the given problem, so it is suggested that the values used be tailored to the task at hand. (Use the mouse to zoom/drag the function.) <select style="background-color: transparent;color: #CC9900; font-family: monospace,monospace; font-weight:bold; line-height: 100%; font-size: 1em;" onchange="optimizerSelection(this)"> <option value="1">&#9654; Optimizer: Adam</option> <option value="2">&#9654; Optimizer: RMSProp</option> <option value="3">&#9654; Optimizer: AdaDelta</option> <option value="4">&#9654; Optimizer: AdaGrad</option> <option value="5">&#9654; Optimizer: CNE</option> <option value="6">&#9654; Optimizer: SMORMS3</option> <option value="7">&#9654; Optimizer: IQN</option> <option value="8">&#9654; Optimizer: CMAES</option> <option value="9">&#9654; Optimizer: AdaMax</option> <option value="10">&#9654; Optimizer: AMSGrad</option> <option value="11">&#9654; Optimizer: Nadam</option> <option value="12">&#9654; Optimizer: SGD</option> <option value="13">&#9654; Optimizer: SGD + Momentum</option> <option value="14">&#9654; Optimizer: L-BFGS</option> <option value="15">&#9654; Optimizer: Gradient descent</option> <option value="16">&#9654; Optimizer: Simulated Annealing</option> <option value="17">&#9654; Optimizer: SPALeRA-SGD</option> <option value="18">&#9654; Optimizer: SVRG</option> <option value="19">&#9654; Optimizer: SVRG-BB</option> <option value="20">&#9654; Optimizer: Katyusha</option> <option value="21">&#9654; Optimizer: Katyusha-Proximal</option> <option value="22">&#9654; Optimizer: SARAH</option> <option value="23">&#9654; Optimizer: SARAH+</option> </select> <button onclick="OptimizerSettingsOpen()" style="position: relative; left: -5px; color: #CC9900; font-family: monospace,monospace; font-weight:bold; line-height: 100%; font-size: 1em; background-color: Transparent; background-repeat:no-repeat; border: none; cursor:pointer; overflow: hidden; outline:none;">&#9654; Settings</button></p></div></div><div> <div id="settingsB" class="expanded"> <div class="wrapperA" style="width:600px; margin:0 auto;"> <div class="a parameterContainerA"> <input id="parameterASlider" type="range" min="0" max="1" step="0.001" onchange="optimizerSettings(this)"/> <p><code style="font-size: 0.8rem;"><span id="parameterADesc">First moment coefficient:</span> <span id="parameterAValue">0.9</span></code></p></div><div class="a parameterContainerB"> <input id="parameterBSlider" type="range" min="0" max="1" step="0.001" onchange="optimizerSettings(this)"/> <p><code style="font-size: 0.8rem;"><span id="parameterBDesc">Second moment coefficient:</span> <span id="parameterBValue">0.999</span></code></p></div><div class="a parameterContainerC"> <input id="parameterCSlider" type="range" min="0" max="1" step="0.001" onchange="optimizerSettings(this)"/> <p><code style="font-size: 0.8rem;"><span id="parameterCDesc">Second moment coefficient:</span> <span id="parameterCValue">0.999</span></code></p></div><div class="a parameterContainerD"> <input id="parameterDSlider" type="range" min="0" max="1" step="0.001" onchange="optimizerSettings(this)"/> <p><code style="font-size: 0.8rem;"><span id="parameterDDesc">Second moment coefficient:</span> <span id="parameterDValue">0.999</span></code></p></div></div></div><p style="color:#F1F1F1;font-size: 0.8rem;">As intuition says, system has higher probability of staying in the states with a smaller stepsize. As the stepsize goes up, imbalance becomes stronger. When the stepsize is close to zero, the system stays in the state(s) with the highest cost.</p><div style="width:100%;"> <canvas id="canvas"></canvas> </div><div class="wrapperA" style="width:600px; margin:0 auto;"> <div class="a"> <br><input id="stepsizeSlider" value="0" type="range" min="0" max="10000" step="1" onchange="evaluationsSliderChanged(this.value)"/> <p><code style="font-size: 0.8rem;"><span id="evaluationDesc">Evaluations:</span> <span id="evaluationValue">Dynamic</span></code></p></div><div class="a"> <br><form> <input id="combine" checked="true" type="checkbox" name="vehicle1" value="Bike" style="position: relative; left: -5px; color: #CC9900; font-family: monospace,monospace; font-weight:bold; line-height: 100%; font-size: 1em; background-color: Transparent; background-repeat:no-repeat; border: none; cursor:pointer; overflow: hidden; outline:none;" onchange="combineSettings(this)"> <code style="font-size: 0.8rem;">Unique Optimizer </code><br><input id="restrict" type="checkbox" name="vehicle1" value="Bike" style="position: relative; left: -5px; color: #CC9900; font-family: monospace,monospace; font-weight:bold; line-height: 100%; font-size: 1em; background-color: Transparent; background-repeat:no-repeat; border: none; cursor:pointer; overflow: hidden; outline:none;" onchange="restricttSettings(this)"> <code style="font-size: 0.8rem;">Restrict Evaluations</code><br></form> </div><div class="a" style="left: 0px; top: 200px;"> <p class="small" style="color:#F1F1F1; font-size: 0.8rem;"> A plot of the cost reveals distinct properties for each optimizer with its own style of convergence. </div></div>
<h1 ><a class="anchor" id="optimizer_optimizertut"></a>
Optimizer</h1>


<h1 ><a class="anchor" id="optimizer_optimizertut"></a>
Optimizer</h1>
<p ><a class="el" href="classmlpack_1_1optimization_1_1AdaDelta.html">AdaDelta</a> - <a class="el" href="classmlpack_1_1optimization_1_1AdaGrad.html">AdaGrad</a> - <a class="el" href="classmlpack_1_1optimization_1_1AdamType.html">Adam</a> - <a class="el" href="classmlpack_1_1optimization_1_1AdaGrad.html">AdaGrad</a> - <a class="el" href="classmlpack_1_1optimization_1_1AdamType.html">Adam</a> - <a class="el" href="classmlpack_1_1optimization_1_1AdamType.html">AdaMax</a> - <a class="el" href="classmlpack_1_1optimization_1_1AdamType.html">AMSGrad</a> - <a class="el" href="classmlpack_1_1optimization_1_1AdamType.html">Nadam</a> - <a class="el" href="classmlpack_1_1optimization_1_1CNE.html">CNE</a> - <a class="el" href="classmlpack_1_1optimization_1_1IQN.html">IQN</a> - <a class="el" href="classmlpack_1_1optimization_1_1L__BFGS.html">L_BFGS</a> - <a class="el" href="classmlpack_1_1optimization_1_1RMSProp.html">RMSProp</a> - <a class="el" href="classmlpack_1_1optimization_1_1SMORMS3.html">SMORMS3</a> - <a class="el" href="classmlpack_1_1optimization_1_1SPALeRASGD.html">SPALeRASGD</a> - <a class="el" href="classmlpack_1_1optimization_1_1SVRGType.html">SVRG</a> - <a class="el" href="classmlpack_1_1optimization_1_1SVRGType.html">SVRG (Barzilai-Borwein)</a> - <a class="el" href="classmlpack_1_1optimization_1_1SARAHType.html">SARAH</a> - <a class="el" href="classmlpack_1_1optimization_1_1SARAHType.html">SARAH+</a> - <a class="el" href="classmlpack_1_1optimization_1_1KatyushaType.html">Katyusha</a> - <a class="el" href="classmlpack_1_1optimization_1_1CMAES.html">CMAES</a></p>
<h2 ><a class="anchor" id="function_type_api_tut"></a>
FunctionType API</h2>
<p >In order to facilitate consistent implementations, we have defined a <code >FunctionType</code> API that describes all the methods that an objective function may implement. <code >mlpack</code> offers a few variations of this API to cover different function characteristics. This leads to several different APIs for different function types:</p>
<ul >
<li ><a class="el" href="optimizertutorial.html#optimizer_functiontype">FunctionType</a>: a normal, differentiable objective function.</li>
<li ><a class="el" href="optimizertutorial.html#optimizer_decomposablefunctiontype">DecomposableFunctionType</a>: a differentiable objective function that can be decomposed into the sum of many objective functions (for SGD-like optimizers).</li>
<li ><a class="el" href="optimizertutorial.html#optimizer_sparsefunctiontype">SparseFunctionType</a>: a decomposable, differentiable objective function with a sparse gradient.</li>
<li ><a class="el" href="optimizertutorial.html#optimizer_nondifferentiablefunctiontype">NonDifferentiableFunctionType</a>: a non-differentiable objective function that can only be evaluated.</li>
<li ><a class="el" href="optimizertutorial.html#optimizer_constrainedfunctiontype">ConstrainedFunctionType</a>: an objective function with constraints on the allowable inputs.</li>
<li ><a class="el" href="optimizertutorial.html#optimizer_nondifferentiabledecomposablefunctiontype">NonDifferentiableDecomposableFunctionType</a>: a decomposable non-differentiable objective function that can only be evaluated.</li>
<li ><a class="el" href="optimizertutorial.html#optimizer_resolvablefunctiontype">ResolvableFunctionType</a>: a differentiable objective function where calculating the gradient with respect to only one parameter is possible.</li>
</ul>
<p >Each of these types of objective functions require slightly different methods to be implemented. In some cases, methods will be automatically deduced by the optimizers using template metaprogramming and this allows the user to not need to implement every method for a given type of objective function. Each type described above is detailed in the following sections.</p>
<h3 ><a class="anchor" id="optimizer_functiontype"></a>
The FunctionType API</h3>
<p >A function satisfying the <code >FunctionType</code> API is a general differentiable objective function. It must implement an <code >Evaluate()</code> and a <code >Gradient()</code> method. The interface used for that can be the following two methods:</p>
<div class="fragment"><div class="line"><span class="comment">// For non-separable objectives.  This should return the objective value for the</span></div><div class="line"><span class="comment">// given parameters.</span></div><div class="line"><span class="keywordtype">double</span> Evaluate(<span class="keyword">const</span> arma::mat&amp; parameters);</div><div class="line"></div><div class="line"><span class="comment">// For non-separable differentiable objectives.  This should store the gradient</span></div><div class="line"><span class="comment">// for the given parameters in the &#39;gradient&#39; matrix.</span></div><div class="line"><span class="keywordtype">void</span> Gradient(<span class="keyword">const</span> arma::mat&amp; parameters, arma::mat&amp; gradient);</div></div><p >However, there are some objective functions (like logistic regression) for which it is computationally convenient to calculate both the objective and the gradient at the same time. Therefore, optionally, the following function can be implemented:</p>
<div class="fragment"><div class="line"><span class="comment">// For non-separable differentiable objectives.  This should store the gradient</span></div><div class="line"><span class="comment">// fro the given parameters in the &#39;gradient&#39; matrix and return the objective</span></div><div class="line"><span class="comment">// value.</span></div><div class="line"><span class="keywordtype">double</span> EvaluateWithGradient(<span class="keyword">const</span> arma::mat&amp; parameters, arma::mat&amp; gradient);</div></div><p >It is not a problem to implement all three of these methods, but it is not obligatory to. <code >EvaluateWithGradient()</code> will automatically be inferred if it is not written from the <code >Evaluate()</code> and <code >Gradient()</code> functions; similarly, the <code >Evaluate()</code> and <code >Gradient()</code> functions will be inferred from <code >EvaluateWithGradient()</code> if they are not available. However, any automatically inferred method may be slightly slower.</p>
<p >The following optimizers use the <code >FunctionType</code> API:</p>
<ul >
<li ><a class="el" href="classmlpack_1_1optimization_1_1LineSearch.html">LineSearch</a></li>
<li ><a class="el" href="classmlpack_1_1optimization_1_1FrankWolfe.html">FrankWolfe</a></li>
<li ><a class="el" href="classmlpack_1_1optimization_1_1GradientDescent.html">GradientDescent</a></li>
<li ><a class="el" href="classmlpack_1_1optimization_1_1L__BFGS.html">L-BFGS</a></li>
</ul>
<h3 ><a class="anchor" id="optimizer_decomposablefunctiontype"></a>
The DecomposableFunctionType API</h3>
<p >A function satisfying the <code >DecomposableFunctionType</code> API is a differentiable objective function that can be decomposed into a number of separable objective functions. Examples of these types of objective functions include those that are a sum of loss on individual data points; so, common machine learning tasks like logistic regression or training a neural network can be expressed as optimizing a decomposable objective function.</p>
<p >Any function implementing the <code >DecomposableFunctionAPI</code> must implement the following four methods:</p>
<div class="fragment"><div class="line"><span class="comment">// For decomposable functions: return the number of parts the optimization</span></div><div class="line"><span class="comment">// problem can be decomposed into.</span></div><div class="line"><span class="keywordtype">size_t</span> NumFunctions();</div><div class="line"></div><div class="line"><span class="comment">// For decomposable objectives.  This should calculate the partial objective</span></div><div class="line"><span class="comment">// starting at the decomposable function indexed by &#39;start&#39; and calculate</span></div><div class="line"><span class="comment">// &#39;batchSize&#39; partial objectives and return the sum.</span></div><div class="line"><span class="keywordtype">double</span> Evaluate(<span class="keyword">const</span> arma::mat&amp; parameters,</div><div class="line">                <span class="keyword">const</span> <span class="keywordtype">size_t</span> start,</div><div class="line">                <span class="keyword">const</span> <span class="keywordtype">size_t</span> batchSize);</div><div class="line"></div><div class="line"><span class="comment">// For separable differentiable objective functions.  This should calculate the</span></div><div class="line"><span class="comment">// gradient starting at the decomposable function indexed by &#39;start&#39; and</span></div><div class="line"><span class="comment">// calculate &#39;batchSize&#39; decomposable gradients and store the sum in the</span></div><div class="line"><span class="comment">// &#39;gradient&#39; matrix.</span></div><div class="line"><span class="keywordtype">void</span> Gradient(<span class="keyword">const</span> arma::mat&amp; parameters,</div><div class="line">              <span class="keyword">const</span> <span class="keywordtype">size_t</span> start,</div><div class="line">              arma::mat&amp; gradient,</div><div class="line">              <span class="keyword">const</span> <span class="keywordtype">size_t</span> batchSize);</div><div class="line"></div><div class="line"><span class="comment">// Shuffle the ordering of the functions.</span></div><div class="line"><span class="keywordtype">void</span> Shuffle();</div></div><p >Note that the decomposable objective functions should support batch computation&mdash;this can allow significant speedups. The <code >Shuffle()</code> method shuffles the ordering of the functions. For some optimizers, randomness is an important component, so it is important that it is possible to shuffle the ordering of the decomposable functions.</p>
<p >As with the regular <code >FunctionType</code> API, it is optional to implement an <code >EvaluateWithGradient()</code> method in place of, or in addition to, the <code >Evaluate()</code> and <code >Gradient()</code> methods. The interface used for that should be the following:</p>
<div class="fragment"><div class="line"><span class="comment">// For decomposable objectives.  This should calculate the partial objective</span></div><div class="line"><span class="comment">// starting at the decomposable function indexed by &#39;start&#39; and calculate</span></div><div class="line"><span class="comment">// &#39;batchSize&#39; partial objectives and return the sum.  This should also</span></div><div class="line"><span class="comment">// calculate the gradient starting at the decomposable function indexed by</span></div><div class="line"><span class="comment">// &#39;start&#39; and calculate &#39;batchSize&#39; decomposable gradients and store the sum in</span></div><div class="line"><span class="comment">// the &#39;gradient&#39; matrix.</span></div><div class="line"><span class="keywordtype">double</span> EvaluateWithGradient(<span class="keyword">const</span> arma::mat&amp; parameters,</div><div class="line">                            <span class="keyword">const</span> <span class="keywordtype">size_t</span> start,</div><div class="line">                            arma::mat&amp; gradient,</div><div class="line">                            <span class="keyword">const</span> <span class="keywordtype">size_t</span> batchSize);</div></div><p >The following mlpack optimizers require functions implementing the <code >DecomposableFunctionType</code> API:</p>
<ul >
<li ><a class="el" href="namespacemlpack_1_1optimization.html#a80af5f3b73ee02a159ca68ac20a50b51">StandardSGD</a></li>
<li ><a class="el" href="namespacemlpack_1_1optimization.html#ae5ae5626c8ca6228b90f6e11c02eb1f8">MomentumSGD</a></li>
<li ><a class="el" href="classmlpack_1_1optimization_1_1AdaDelta.html">AdaDelta</a></li>
<li ><a class="el" href="classmlpack_1_1optimization_1_1AdaGrad.html">AdaGrad</a></li>
<li ><a class="el" href="namespacemlpack_1_1optimization.html#ae015766100b47037db7b997462facc97">Adam</a></li>
<li ><a class="el" href="namespacemlpack_1_1optimization.html#a7d6274d40de7c01efc1ae34158a319cc">AdaMax</a></li>
<li ><a class="el" href="namespacemlpack_1_1optimization.html#a07b6ef20451d6c19952d44ff6c52e3bb">AMSGrad</a></li>
<li ><a class="el" href="namespacemlpack_1_1optimization.html#a373a947f96aee47a23b0f24a9760c2a4">Nadam</a></li>
<li ><a class="el" href="namespacemlpack_1_1optimization.html#ad8acb27595de9dc717961e6e9b0d89bd">NadaMax</a></li>
<li ><a class="el" href="classmlpack_1_1optimization_1_1IQN.html">IQN</a></li>
<li ><a class="el" href="namespacemlpack_1_1optimization.html#a06530afc92ee02d33b08e8c32860bee6">Katyusha</a></li>
<li ><a class="el" href="namespacemlpack_1_1optimization.html#a6654a0baa3bab4b02d9b6e5eb901397f">KatyushaProximal</a></li>
<li ><a class="el" href="classmlpack_1_1optimization_1_1RMSProp.html">RMSProp</a></li>
<li ><a class="el" href="namespacemlpack_1_1optimization.html#af3f51b1e39d7b8c88aef4c0a3cd31ddd">SARAH</a></li>
<li ><a class="el" href="namespacemlpack_1_1optimization.html#ae0bcb29037b15bffb25fb75d31e2e4a5">SARAH+</a></li>
<li ><a class="el" href="classmlpack_1_1optimization_1_1SGDR.html">SGDR</a></li>
<li ><a class="el" href="classmlpack_1_1optimization_1_1SMORMS3.html">SMORMS3</a></li>
<li ><a class="el" href="classmlpack_1_1optimization_1_1SPALeRASGD.html">SPALeRA SGD</a></li>
<li ><a class="el" href="namespacemlpack_1_1optimization.html#a8cf60766bdebcfd7ce8219c2a5ca8f36">SVRG</a></li>
<li ><a class="el" href="namespacemlpack_1_1optimization.html#aa8806a595ca5c13a9dabc67c4ef86f24">SVRG-BB</a></li>
</ul>
<h3 ><a class="anchor" id="optimizer_sparsefunctiontype"></a>
The SparseFunctionType API</h3>
<p >A function satisfying the <code >SparseFunctionType</code> API is a decomposable differentiable objective function with the condition that a single individual gradient is sparse. The API is slightly different but similar to the <code >DecomposableFunctionType</code> API; the following methods are necessary:</p>
<div class="fragment"><div class="line"><span class="comment">// For decomposable functions: return the number of parts the optimization</span></div><div class="line"><span class="comment">// problem can be decomposed into.</span></div><div class="line"><span class="keywordtype">size_t</span> NumFunctions();</div><div class="line"></div><div class="line"><span class="comment">// For decomposable objectives.  This should calculate the partial objective</span></div><div class="line"><span class="comment">// starting at the decomposable function indexed by &#39;start&#39; and calculate</span></div><div class="line"><span class="comment">// &#39;batchSize&#39; partial objectives and return the sum.</span></div><div class="line"><span class="keywordtype">double</span> Evaluate(<span class="keyword">const</span> arma::mat&amp; parameters,</div><div class="line">                <span class="keyword">const</span> <span class="keywordtype">size_t</span> start,</div><div class="line">                <span class="keyword">const</span> <span class="keywordtype">size_t</span> batchSize);</div><div class="line"></div><div class="line"><span class="comment">// For separable differentiable objective functions.  This should calculate the</span></div><div class="line"><span class="comment">// gradient starting at the decomposable function indexed by &#39;start&#39; and</span></div><div class="line"><span class="comment">// calculate &#39;batchSize&#39; decomposable gradients and store the sum in the</span></div><div class="line"><span class="comment">// &#39;gradient&#39; matrix, which is a sparse matrix.</span></div><div class="line"><span class="keywordtype">void</span> Gradient(<span class="keyword">const</span> arma::mat&amp; parameters,</div><div class="line">              <span class="keyword">const</span> <span class="keywordtype">size_t</span> start,</div><div class="line">              arma::sp_mat&amp; gradient,</div><div class="line">              <span class="keyword">const</span> <span class="keywordtype">size_t</span> batchSize);</div></div><p >The <code >Shuffle()</code> method is not needed for the <code >SparseFunctionType</code> API.</p>
<p >Note that it is possible to write a <code >Gradient()</code> method that accepts a template parameter <code >GradType</code>, which may be <code >arma::mat</code> (dense Armadillo matrix) or <code >arma::sp_mat</code> (sparse Armadillo matrix). This allows support for both the <code >DecomposableFunctionType</code> API and the <code >SparseFunctionType</code> API, as below:</p>
<div class="fragment"><div class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> GradType&gt;</div><div class="line"><span class="keywordtype">void</span> Gradient(<span class="keyword">const</span> arma::mat&amp; parameters,</div><div class="line">              <span class="keyword">const</span> <span class="keywordtype">size_t</span> start,</div><div class="line">              GradType&amp; gradient,</div><div class="line">              <span class="keyword">const</span> <span class="keywordtype">size_t</span> batchSize);</div></div><p >The following mlpack optimizers require an objective function satisfying the <code >SparseFunctionType</code> API:</p>
<ul >
<li ><a class="el" href="classmlpack_1_1optimization_1_1ParallelSGD.html">ParallelSGD</a></li>
</ul>
<h3 ><a class="anchor" id="optimizer_nondifferentiablefunctiontype"></a>
The NonDifferentiableFunctionType API</h3>
<p >A function satisfying the <code >NonDifferentiableFunctionType</code> API is a general non-differentiable objective function. Only an <code >Evaluate()</code> method must be implemented, with the following signature:</p>
<div class="fragment"><div class="line"><span class="comment">// For non-separable objectives.  This should return the objective value for the</span></div><div class="line"><span class="comment">// given parameters.</span></div><div class="line"><span class="keywordtype">double</span> Evaluate(<span class="keyword">const</span> arma::mat&amp; parameters);</div></div><p >The following mlpack optimizers require an objective function satisfying the <code >NonDifferentiableFunctionType</code> API:</p>
<ul >
<li ><a class="el" href="classmlpack_1_1optimization_1_1SA.html">Simulated Annealing</a></li>
</ul>
<h3 ><a class="anchor" id="optimizer_constrainedfunctiontype"></a>
The ConstrainedFunctionType API</h3>
<p >A function satisfying the <code >ConstrainedFunctionType</code> API is a general differentiable objective function that has differentiable constraints for the parameters. This API is more complex than the others and requires five methods to be implemented:</p>
<div class="fragment"><div class="line"><span class="comment">// For non-separable objectives.  This should return the objective value for the</span></div><div class="line"><span class="comment">// given parameters.</span></div><div class="line"><span class="keywordtype">double</span> Evaluate(<span class="keyword">const</span> arma::mat&amp; parameters);</div><div class="line"></div><div class="line"><span class="comment">// For non-separable differentiable objectives.  This should store the gradient</span></div><div class="line"><span class="comment">// for the given parameters in the &#39;gradient&#39; matrix.</span></div><div class="line"><span class="keywordtype">void</span> Gradient(<span class="keyword">const</span> arma::mat&amp; parameters, arma::mat&amp; gradient);</div><div class="line"></div><div class="line"><span class="comment">// Return the number of constraints.</span></div><div class="line"><span class="keywordtype">size_t</span> NumConstraints();</div><div class="line"></div><div class="line"><span class="comment">// Evaluate the constraint with the given index.</span></div><div class="line"><span class="keywordtype">double</span> EvaluateConstraint(<span class="keyword">const</span> <span class="keywordtype">size_t</span> index, <span class="keyword">const</span> arma::mat&amp; parameters);</div><div class="line"></div><div class="line"><span class="comment">// Store the gradient of the constraint with the given index in the &#39;gradient&#39;</span></div><div class="line"><span class="comment">// matrix.</span></div><div class="line"><span class="keywordtype">void</span> GradientConstraint(<span class="keyword">const</span> <span class="keywordtype">size_t</span> index,</div><div class="line">                        <span class="keyword">const</span> arma::mat&amp; parameters,</div><div class="line">                        arma::mat&amp; gradient);</div></div><p >The following mlpack optimizers require a <code >ConstrainedFunctionType:</code> </p>
<ul >
<li ><a class="el" href="classmlpack_1_1optimization_1_1AugLagrangian.html">AugLagrangian</a></li>
</ul>
<h3 ><a class="anchor" id="optimizer_nondifferentiabledecomposablefunctiontype"></a>
The NonDifferentiableDecomposableFunctionType API</h3>
<p >A function satisfying the <code >NonDifferentiableDecomposableFunctionType</code> API is a decomposable non-differentiable objective function. Only an <code >Evaluate()</code> and a <code >NumFunctions()</code> method must be implemented, with the following signatures:</p>
<div class="fragment"><div class="line"><span class="comment">// For decomposable functions: return the number of parts the optimization</span></div><div class="line"><span class="comment">// problem can be decomposed into.</span></div><div class="line"><span class="keywordtype">size_t</span> NumFunctions();</div><div class="line"></div><div class="line"><span class="comment">// For decomposable objectives.  This should calculate the partial objective</span></div><div class="line"><span class="comment">// starting at the decomposable function indexed by &#39;start&#39; and calculate</span></div><div class="line"><span class="comment">// &#39;batchSize&#39; partial objectives and return the sum.</span></div><div class="line"><span class="keywordtype">double</span> Evaluate(<span class="keyword">const</span> arma::mat&amp; parameters,</div><div class="line">                <span class="keyword">const</span> <span class="keywordtype">size_t</span> start,</div><div class="line">                <span class="keyword">const</span> <span class="keywordtype">size_t</span> batchSize);</div></div><p >The following mlpack optimizers require a <code >NonDifferentiableDecomposableFunctionType:</code> </p>
<ul >
<li ><a class="el" href="namespacemlpack_1_1optimization.html#a4a29a6f28bda38f78a6068c898c7c20d">ApproxCMAES</a></li>
<li ><a class="el" href="classmlpack_1_1optimization_1_1CMAES.html">CMAES</a></li>
<li ><a class="el" href="classmlpack_1_1optimization_1_1CNE.html">CNE</a></li>
</ul>
<h3 ><a class="anchor" id="optimizer_resolvablefunctiontype"></a>
The ResolvableFunctionType API</h3>
<p >A function satisfying the <code >ResolvableFunctionType</code> API is a partially differentiable objective function. For this API, three methods must be implemented, with the following signatures:</p>
<div class="fragment"><div class="line"><span class="comment">// For partially differentiable functions: return the number of partial</span></div><div class="line"><span class="comment">// derivatives.</span></div><div class="line"><span class="keywordtype">size_t</span> NumFeatures();</div><div class="line"></div><div class="line"><span class="comment">// For non-separable objectives.  This should return the objective value for the</span></div><div class="line"><span class="comment">// given parameters.</span></div><div class="line"><span class="keywordtype">double</span> Evaluate(<span class="keyword">const</span> arma::mat&amp; parameters);</div><div class="line"></div><div class="line"><span class="comment">// For partially differentiable sparse and non-sparse functions.  Store the</span></div><div class="line"><span class="comment">// given partial gradient for the parameter index &#39;j&#39; in the &#39;gradient&#39; matrix.</span></div><div class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> GradType&gt;</div><div class="line"><span class="keywordtype">void</span> PartialGradient(<span class="keyword">const</span> arma::mat&amp; parameters,</div><div class="line">                     <span class="keyword">const</span> <span class="keywordtype">size_t</span> j,</div><div class="line">                     GradType&amp; gradient);</div></div><p >It is not required to templatize so that both sparse and dense gradients can be used, but it can be helpful.</p>
<p >The following mlpack optimizers require a <code >ResolvableFunctionType:</code> </p>
<ul >
<li ><a class="el" href="classmlpack_1_1optimization_1_1SCD.html">SCD</a></li>
</ul>
<h2 ><a class="anchor" id="optimizer_type_api_tut"></a>
Optimizer API</h2>
<p >An optimizer must implement only the method:</p>
<div class="fragment"><div class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> FunctionType&gt;</div><div class="line"><span class="keywordtype">double</span> Optimize(FunctionType&amp; <span class="keyword">function</span>, arma::mat&amp; parameters);</div></div><p >The <code >Optimize()</code> method optimizes the given function <code >function</code>, and stores the best set of parameters in the matrix <code >parameters</code> and returns the best objective value.</p>
<p >If the optimizer requires a given API from above, the following functions from <code >src/mlpack/optimizers/function/static_checks.hpp</code> can be helpful:</p>
<ul >
<li ><a class="el" href="namespacemlpack_1_1optimization_1_1traits.html#a5dd0af6f3015f270703eac7919101222" title="Perform checks for the regular FunctionType API. ">mlpack::optimization::traits::CheckFunctionTypeAPI()</a></li>
<li ><a class="el" href="namespacemlpack_1_1optimization_1_1traits.html#a1c9ee8af6a35fcc2d231874baf726dca" title="Perform checks for the DecomposableFunctionType API. ">mlpack::optimization::traits::CheckDecomposableFunctionTypeAPI()</a></li>
<li ><a class="el" href="namespacemlpack_1_1optimization_1_1traits.html#a141428964f776db1c6720d4628082006" title="Perform checks for the SparseFunctionType API. ">mlpack::optimization::traits::CheckSparseFunctionTypeAPI()</a></li>
<li ><a class="el" href="namespacemlpack_1_1optimization_1_1traits.html#a6de576d325c10e629f3a22603abe6897" title="Perform checks for the NonDifferentiableFunctionType API. ">mlpack::optimization::traits::CheckNonDifferentiableFunctionTypeAPI()</a></li>
<li ><a class="el" href="namespacemlpack_1_1optimization_1_1traits.html#af6bd66bcf224107430fe0a737e339de3" title="Perform checks for the ConstrainedFunctionType API. ">mlpack::optimization::traits::CheckConstrainedFunctionTypeAPI()</a></li>
<li ><a class="el" href="namespacemlpack_1_1optimization_1_1traits.html#ae7f7128cff8a584136beda077d49cb66" title="Perform checks for the NonDifferentiableDecomposableFunctionType API. ">mlpack::optimization::traits::CheckNonDifferentiableDecomposableFunctionTypeAPI()</a></li>
<li ><a class="el" href="namespacemlpack_1_1optimization_1_1traits.html#a127d5706aa2ff05f7fce9f8878450624" title="Perform checks for the ResolvableFunctionType API. ">mlpack::optimization::traits::CheckResolvableFunctionTypeAPI()</a></li>
</ul>
<h2 ><a class="anchor" id="cpp_ex1_optimizer_tut"></a>
Simple Function and Optimizer example</h2>
<p >The example below constructs a simple function, where each dimension has a parabola with a distinct minimum. Note, in this example we maintain an ordering with the vector <code >order</code>; in other situations, such as training neural networks, we could simply shuffle the columns of the data matrix in <code >Shuffle()</code>.</p>
<div class="fragment"><div class="line"><span class="keyword">class </span>ObjectiveFunction</div><div class="line">{</div><div class="line"> <span class="keyword">public</span>:</div><div class="line">  <span class="comment">// A separable function consisting of four quadratics.</span></div><div class="line">  ObjectiveFunction()</div><div class="line">  {</div><div class="line">    in = arma::vec(<span class="stringliteral">&quot;20 12 15 100&quot;</span>);</div><div class="line">    bi = arma::vec(<span class="stringliteral">&quot;-4 -2 -3 -8&quot;</span>);</div><div class="line">  }</div><div class="line"></div><div class="line">  <span class="keywordtype">size_t</span> NumFunctions() { <span class="keywordflow">return</span> 4; }</div><div class="line">  <span class="keywordtype">void</span> Shuffle() { ord = arma::shuffle(arma::uvec(<span class="stringliteral">&quot;0 1 2 3&quot;</span>)); }</div><div class="line"></div><div class="line">  <span class="keywordtype">double</span> Evaluate(<span class="keyword">const</span> arma::mat&amp; para, <span class="keyword">const</span> <span class="keywordtype">size_t</span> s, <span class="keyword">const</span> <span class="keywordtype">size_t</span> bs)</div><div class="line">  {</div><div class="line">    <span class="keywordtype">double</span> cost = 0;</div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> i = s; i &lt; s + bs; i++)</div><div class="line">      cost += para(ord[i]) * para(ord[i]) + bi(ord[i]) * para(ord[i]) + in(ord[i]);</div><div class="line">    <span class="keywordflow">return</span> cost;</div><div class="line">  }</div><div class="line"></div><div class="line">  <span class="keywordtype">void</span> Gradient(<span class="keyword">const</span> arma::mat&amp; para, <span class="keyword">const</span> <span class="keywordtype">size_t</span> s, arma::mat&amp; g, <span class="keyword">const</span> <span class="keywordtype">size_t</span> bs)</div><div class="line">  {</div><div class="line">    g.zeros(para.n_rows, para.n_cols);</div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> i = s; i &lt; s + bs; i++)</div><div class="line">      g(ord[i]) += (1.0 / bs) * 2 * para(ord[i]) + bi(ord[i]);</div><div class="line">  }</div><div class="line"></div><div class="line"> <span class="keyword">private</span>:</div><div class="line">  <span class="comment">// Intercepts.</span></div><div class="line">  arma::vec in;</div><div class="line"></div><div class="line">  <span class="comment">// Coefficient.</span></div><div class="line">  arma::vec bi;</div><div class="line"></div><div class="line">  <span class="comment">// Function order.</span></div><div class="line">  arma::uvec ord;</div><div class="line">};</div></div><p >For the optimization of the defined <code >ObjectiveFunction</code> and other <code >mlpack</code> objective functions, we must implement only an Optimize() method, and a constructor to set some parameters. The code is given below.</p>
<div class="fragment"><div class="line"><span class="keyword">class </span>SimpleOptimizer</div><div class="line">{</div><div class="line"> <span class="keyword">public</span>:</div><div class="line">  SimpleOptimizer(<span class="keyword">const</span> <span class="keywordtype">size_t</span> bs = 1, <span class="keyword">const</span> <span class="keywordtype">double</span> lr = 0.02) : bs(bs), lr(lr) { }</div><div class="line"></div><div class="line">  <span class="keyword">template</span>&lt;<span class="keyword">typename</span> FunctionType&gt;</div><div class="line">  <span class="keywordtype">double</span> Optimize(FunctionType&amp; <span class="keyword">function</span>, arma::mat&amp; parameter)</div><div class="line">  {</div><div class="line">    arma::mat gradient;</div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> i = 0; i &lt; 5000; i += bs)</div><div class="line">    {</div><div class="line">      <span class="keywordflow">if</span> (i % <span class="keyword">function</span>.NumFunctions() == 0)</div><div class="line">      {</div><div class="line">        <span class="keyword">function</span>.Shuffle();</div><div class="line">      }</div><div class="line"></div><div class="line">      <span class="keyword">function</span>.Gradient(parameter, i % <span class="keyword">function</span>.NumFunctions(), gradient, bs);</div><div class="line">      parameter -= lr * gradient;</div><div class="line">    }</div><div class="line"></div><div class="line">    <span class="keywordflow">return</span> <span class="keyword">function</span>.Evaluate(parameter, 0, <span class="keyword">function</span>.NumFunctions());</div><div class="line">  }</div><div class="line"> <span class="keyword">private</span>:</div><div class="line">  <span class="keywordtype">size_t</span> bs;</div><div class="line"></div><div class="line">  <span class="keywordtype">double</span> lr;</div><div class="line">};</div></div><p >Note for the sake of simplicity we omitted checks on the batch size (<code >bs</code>). This optimizer assumes that <code >function.NumFunctions()</code> is a multiple of the batch size, we also omitted other typical parts of real implementations, a more detailed example may be found in the <a class="el" href="namespacemlpack_1_1optimization.html">complete API</a> documentation". Still, <code >SimpleOptimizer</code> works with any mlpack objective function which implements  Evaluate that can compute a partial objective function, and <code >Gradient</code> that can compute a part of the gradient starting with a separable function.</p>
<p >Finding the minimum of <code >ObjectiveFunction</code> or any other <code >mlpack</code> function can be done as shown below.</p>
<div class="fragment"><div class="line">ObjectiveFunction <span class="keyword">function</span>;</div><div class="line">arma::mat parameter(<span class="stringliteral">&quot;0 0 0 0;&quot;</span>);</div><div class="line"></div><div class="line">SimpleOptimizer optimizer;</div><div class="line"><span class="keywordtype">double</span> objective = optimizer.Optimize(<span class="keyword">function</span>, parameter);</div><div class="line"></div><div class="line">std::cout &lt;&lt; <span class="stringliteral">&quot;Objective: &quot;</span> &lt;&lt; objective &lt;&lt; std::endl;</div><div class="line">std::cout &lt;&lt; <span class="stringliteral">&quot;Optimized function parameter: &quot;</span> &lt;&lt; parameter &lt;&lt; std::endl;</div></div><p >The final value of the objective function should be close to the optimal value, which is the sum of values at the vertices of the parabolas.</p>
<h1 ><a class="anchor" id="further_doc_optimizer_tut"></a>
Further documentation</h1>
<p >Further documentation for each <code >Optimizer</code> may be found in the <a class="el" href="namespacemlpack_1_1optimization.html">complete API documentation</a>. In addition, more information on the testing functions may be found in its <a class="el" href="namespacemlpack_1_1optimization.html">complete API documentation</a>. </p>
</div></div>

<hr class="footer"></hr><address class="footer"><small >
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"></img>
</a> 1.8.13
</small></address>
</div>
</body>
<script type="text/javascript">
var x = document.querySelectorAll("img.formulaDsp");
var i;
for (i = 0; i < x.length; i++)
{
  x[i].width = x[i].offsetWidth / 4;
}
</script>
</html