<html >
<head >

<meta name="keywords" content="mlpack, libmlpack, c++, armadillo, machine
learning, data mining, classification, regression, tree-based methods, dual-tree
algorithm">
<meta name="description" content="mlpack: a scalable c++ machine learning
library">
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title >mlpack: a scalable c++ machine learning library</title>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript" src="dynamic_tables.js"></script>
</head><link rel="stylesheet" href="style-doxygen.css" /><link rel="stylesheet" href="doxygen.css" /><link rel="stylesheet" href="tabs.css" /><link rel="stylesheet" href="search/search.css" /><link href="http://fonts.googleapis.com/css?family=Maven+Pro:500" rel="stylesheet" type="text/css" />





<body ><br />


<div class="mlpack_titlebar">
   <a href="http://www.mlpack.org"><img src="../../../mlpack.png"></a>
</div>
<center >
<div class="mlnavbar">
  <div class="navcontainer">
   <div class="mlnavitem" name="mlnavmain"><a href="../../../index.html">main</a></div>
   <div class="mlnavitem" name="mlnavabout"><a href="../../../about.html">about</a></div>
   <div class="mlnavitem" name="mlnavdoc"><a href="../../../docs.html">docs</a></div>
   <div class="mlnavitem" name="mlnavhelp"><a href="../../../help.html">get help</a></div>
   <div class="mlnavitem" name="mlnavbugs"><a href="https://github.com/mlpack/mlpack">github</a></div>
  </div>
</div>
</center>
<div class="separator"></div>
<center >
<div class="mainsection smallertext">
<div id="top">
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody >
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">mlpack
   &#160;<span id="projectnumber">3.0.1</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>

<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>

<div id="MSearchSelectWindow" onmouseover="return searchBox.OnSearchSelectShow()" onmouseout="return searchBox.OnSearchSelectHide()" onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>


<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul >
<li class="navelem"><a class="el" href="namespacemlpack.html">mlpack</a></li><li class="navelem"><a class="el" href="namespacemlpack_1_1optimization.html">optimization</a></li>  </ul>
</div>
</div>
<div class="header">
  <div class="summary">
<a href="#namespaces">Namespaces</a> &#124;
<a href="#nested-classes">Classes</a> &#124;
<a href="#typedef-members">Typedefs</a>  </div>
  <div class="headertitle">
<div class="title">mlpack::optimization Namespace Reference</div>  </div>
</div>
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="namespaces"></a>
Namespaces</h2></td></tr>
<tr class="memitem:namespacemlpack_1_1optimization_1_1aux"><td class="memItemLeft" align="right" valign="top"> &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacemlpack_1_1optimization_1_1aux.html">aux</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:namespacemlpack_1_1optimization_1_1test"><td class="memItemLeft" align="right" valign="top"> &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacemlpack_1_1optimization_1_1test.html">test</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:namespacemlpack_1_1optimization_1_1traits"><td class="memItemLeft" align="right" valign="top"> &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacemlpack_1_1optimization_1_1traits.html">traits</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="nested-classes"></a>
Classes</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AdaDelta.html">AdaDelta</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight"><a class="el" href="classmlpack_1_1optimization_1_1AdaDelta.html" title="AdaDelta is an optimizer that uses two ideas to improve upon the two main drawbacks of the Adagrad me...">AdaDelta</a> is an optimizer that uses two ideas to improve upon the two main drawbacks of the Adagrad method:  <a href="classmlpack_1_1optimization_1_1AdaDelta.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AdaDeltaUpdate.html">AdaDeltaUpdate</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Implementation of the <a class="el" href="classmlpack_1_1optimization_1_1AdaDelta.html" title="AdaDelta is an optimizer that uses two ideas to improve upon the two main drawbacks of the Adagrad me...">AdaDelta</a> update policy.  <a href="classmlpack_1_1optimization_1_1AdaDeltaUpdate.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AdaGrad.html">AdaGrad</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight"><a class="el" href="classmlpack_1_1optimization_1_1AdaGrad.html" title="AdaGrad is a modified version of stochastic gradient descent which performs larger updates for more s...">AdaGrad</a> is a modified version of stochastic gradient descent which performs larger updates for more sparse parameters and smaller updates for less sparse parameters.  <a href="classmlpack_1_1optimization_1_1AdaGrad.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AdaGradUpdate.html">AdaGradUpdate</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Implementation of the <a class="el" href="classmlpack_1_1optimization_1_1AdaGrad.html" title="AdaGrad is a modified version of stochastic gradient descent which performs larger updates for more s...">AdaGrad</a> update policy.  <a href="classmlpack_1_1optimization_1_1AdaGradUpdate.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AdaMaxUpdate.html">AdaMaxUpdate</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">AdaMax is a variant of Adam, an optimizer that computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients.based on the infinity norm as given in the section 7 of the following paper.  <a href="classmlpack_1_1optimization_1_1AdaMaxUpdate.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AdamType.html">AdamType</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Adam is an optimizer that computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients.  <a href="classmlpack_1_1optimization_1_1AdamType.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AdamUpdate.html">AdamUpdate</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Adam is an optimizer that computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients as given in the section 7 of the following paper.  <a href="classmlpack_1_1optimization_1_1AdamUpdate.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AdaptiveStepsize.html">AdaptiveStepsize</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Definition of the adaptive stepize technique, a non-monotonic stepsize scheme that uses curvature estimates to propose new stepsize choices.  <a href="classmlpack_1_1optimization_1_1AdaptiveStepsize.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluate.html">AddDecomposableEvaluate</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">The <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluate.html" title="The AddDecomposableEvaluate mixin class will add a decomposable Evaluate() method if a decomposable E...">AddDecomposableEvaluate</a> mixin class will add a decomposable <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluate.html#a20f3a2def7de4e78157e4b90adcf780d">Evaluate()</a> method if a decomposable EvaluateWithGradient() function exists, or nothing otherwise.  <a href="classmlpack_1_1optimization_1_1AddDecomposableEvaluate.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluate_3_01FunctionType_00_01HasDecomposableEvaluateWithGradient_00_01true_01_4.html">AddDecomposableEvaluate&lt; FunctionType, HasDecomposableEvaluateWithGradient, true &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Reflect the existing <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluate_3_01FunctionType_00_01HasDecomposableEvaluateWithGradient_00_01true_01_4.html#a8105e9787325b33a9d572ff9d1b6d33e">Evaluate()</a>.  <a href="classmlpack_1_1optimization_1_1AddDecomposableEvaluate_3_01FunctionType_00_01HasDecomposableEvaluateWithGradient_00_01true_01_4.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluate_3_01FunctionType_00_01true_00_01false_01_4.html">AddDecomposableEvaluate&lt; FunctionType, true, false &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">If we have a decomposable EvaluateWithGradient() but not a decomposable <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluate_3_01FunctionType_00_01true_00_01false_01_4.html#a8105e9787325b33a9d572ff9d1b6d33e" title="Return the objective function for the given coordinates, starting at the given decomposable function ...">Evaluate()</a>, add a decomposable <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluate_3_01FunctionType_00_01true_00_01false_01_4.html#a8105e9787325b33a9d572ff9d1b6d33e" title="Return the objective function for the given coordinates, starting at the given decomposable function ...">Evaluate()</a> method.  <a href="classmlpack_1_1optimization_1_1AddDecomposableEvaluate_3_01FunctionType_00_01true_00_01false_01_4.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateConst.html">AddDecomposableEvaluateConst</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">The <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateConst.html" title="The AddDecomposableEvaluateConst mixin class will add a decomposable const Evaluate() method if a dec...">AddDecomposableEvaluateConst</a> mixin class will add a decomposable const <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateConst.html#ad050970d1db927f28a29ee2ff99169c0">Evaluate()</a> method if a decomposable const EvaluateWithGradient() function exists, or nothing otherwise.  <a href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateConst.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateConst_3_01FunctionType_00_01HasDecomposabl52dd2e79dcc076322580942f0b737881.html">AddDecomposableEvaluateConst&lt; FunctionType, HasDecomposableEvaluateWithGradient, true &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Reflect the existing <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateConst_3_01FunctionType_00_01HasDecomposabl52dd2e79dcc076322580942f0b737881.html#a639afd431ea514ba5fbab58592bfb927">Evaluate()</a>.  <a href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateConst_3_01FunctionType_00_01HasDecomposabl52dd2e79dcc076322580942f0b737881.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateConst_3_01FunctionType_00_01true_00_01false_01_4.html">AddDecomposableEvaluateConst&lt; FunctionType, true, false &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">If we have a decomposable const EvaluateWithGradient() but not a decomposable const <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateConst_3_01FunctionType_00_01true_00_01false_01_4.html#a639afd431ea514ba5fbab58592bfb927" title="Return the objective function for the given coordinates, starting at the given decomposable function ...">Evaluate()</a>, add a decomposable const <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateConst_3_01FunctionType_00_01true_00_01false_01_4.html#a639afd431ea514ba5fbab58592bfb927" title="Return the objective function for the given coordinates, starting at the given decomposable function ...">Evaluate()</a> method.  <a href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateConst_3_01FunctionType_00_01true_00_01false_01_4.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateStatic.html">AddDecomposableEvaluateStatic</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">The <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateStatic.html" title="The AddDecomposableEvaluateStatic mixin class will add a decomposable static Evaluate() method if a d...">AddDecomposableEvaluateStatic</a> mixin class will add a decomposable static <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateStatic.html#a16e0dab117bf75fb4818160af94716f2">Evaluate()</a> method if a decomposable static EvaluateWithGradient() function exists, or nothing otherwise.  <a href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateStatic.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateStatic_3_01FunctionType_00_01HasDecomposabcd2c84137b7dd7acf0932436bffeefbd.html">AddDecomposableEvaluateStatic&lt; FunctionType, HasDecomposableEvaluateWithGradient, true &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Reflect the existing <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateStatic_3_01FunctionType_00_01HasDecomposabcd2c84137b7dd7acf0932436bffeefbd.html#a2b2d126fd7ebda8fcc22ec82111425e9">Evaluate()</a>.  <a href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateStatic_3_01FunctionType_00_01HasDecomposabcd2c84137b7dd7acf0932436bffeefbd.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateStatic_3_01FunctionType_00_01true_00_01false_01_4.html">AddDecomposableEvaluateStatic&lt; FunctionType, true, false &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">If we have a decomposable EvaluateWithGradient() but not a decomposable <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateStatic_3_01FunctionType_00_01true_00_01false_01_4.html#a2b2d126fd7ebda8fcc22ec82111425e9" title="Return the objective function for the given coordinates, starting at the given decomposable function ...">Evaluate()</a>, add a decomposable <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateStatic_3_01FunctionType_00_01true_00_01false_01_4.html#a2b2d126fd7ebda8fcc22ec82111425e9" title="Return the objective function for the given coordinates, starting at the given decomposable function ...">Evaluate()</a> method.  <a href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateStatic_3_01FunctionType_00_01true_00_01false_01_4.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradient.html">AddDecomposableEvaluateWithGradient</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">The <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradient.html" title="The AddDecomposableEvaluateWithGradient mixin class will add a decomposable EvaluateWithGradient() me...">AddDecomposableEvaluateWithGradient</a> mixin class will add a decomposable <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradient.html#aa2b53c3a9ff8c418af39f4648a67b1c3">EvaluateWithGradient()</a> method if a decomposable Evaluate() method and a decomposable Gradient() method exists, or nothing otherwise.  <a href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradient.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradient_3_01FunctionType_00_01false_00_01true_00_01true_01_4.html">AddDecomposableEvaluateWithGradient&lt; FunctionType, false, true, true &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">If the FunctionType has EvaluateWithGradient() but not <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradient_3_01FunctionType_00_01false_00_01true_00_01true_01_4.html#a8105e9787325b33a9d572ff9d1b6d33e" title="Return the objective function for the given coordinates. ">Evaluate()</a>, provide that function.  <a href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradient_3_01FunctionType_00_01false_00_01true_00_01true_01_4.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradient_3_01FunctionType_00_01HasDeco09f073cc9c87ad2ec2f53f997e8db409.html">AddDecomposableEvaluateWithGradient&lt; FunctionType, HasDecomposableEvaluateGradient, true &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Reflect the existing <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradient_3_01FunctionType_00_01HasDeco09f073cc9c87ad2ec2f53f997e8db409.html#a767c267665093ae33a57ca4649870940">EvaluateWithGradient()</a>.  <a href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradient_3_01FunctionType_00_01HasDeco09f073cc9c87ad2ec2f53f997e8db409.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradient_3_01FunctionType_00_01true_00_01false_01_4.html">AddDecomposableEvaluateWithGradient&lt; FunctionType, true, false &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">If we have a both decomposable Evaluate() and a decomposable Gradient() but not a decomposable <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradient_3_01FunctionType_00_01true_00_01false_01_4.html#a767c267665093ae33a57ca4649870940" title="Return both the evaluated objective function and its gradient, storing the gradient in the given matr...">EvaluateWithGradient()</a>, add a decomposable <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradient_3_01FunctionType_00_01true_00_01false_01_4.html#a767c267665093ae33a57ca4649870940" title="Return both the evaluated objective function and its gradient, storing the gradient in the given matr...">EvaluateWithGradient()</a> method.  <a href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradient_3_01FunctionType_00_01true_00_01false_01_4.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradient_3_01FunctionType_00_01true_00_01false_00_01true_01_4.html">AddDecomposableEvaluateWithGradient&lt; FunctionType, true, false, true &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">If the FunctionType has EvaluateWithGradient() but not <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradient_3_01FunctionType_00_01true_00_01false_00_01true_01_4.html#a5fa8c51329cc29fc2ee7db1b4a3492c6" title="Calculate the gradient and store it in the given matrix. ">Gradient()</a>, provide that function.  <a href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradient_3_01FunctionType_00_01true_00_01false_00_01true_01_4.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradient_3_01FunctionType_00_01true_00_01true_00_01false_01_4.html">AddDecomposableEvaluateWithGradient&lt; FunctionType, true, true, false &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">If the FunctionType has Evaluate() and Gradient() but not <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradient_3_01FunctionType_00_01true_00_01true_00_01false_01_4.html#a767c267665093ae33a57ca4649870940" title="Return both the evaluated objective function and its gradient, storing the gradient in the given matr...">EvaluateWithGradient()</a>, we will provide the latter.  <a href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradient_3_01FunctionType_00_01true_00_01true_00_01false_01_4.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradientConst.html">AddDecomposableEvaluateWithGradientConst</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">The <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradientConst.html" title="The AddDecomposableEvaluateWithGradientConst mixin class will add a decomposable const EvaluateWithGr...">AddDecomposableEvaluateWithGradientConst</a> mixin class will add a decomposable const <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradientConst.html#adcc8e455888c3c15d898d96f95af4ecf">EvaluateWithGradient()</a> method if both a decomposable const Evaluate() and a decomposable const Gradient() function exist, or nothing otherwise.  <a href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradientConst.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradientConst_3_01FunctionType_00_01Ha2a2b96a2e864e9694c577ab3c86846da.html">AddDecomposableEvaluateWithGradientConst&lt; FunctionType, HasDecomposableEvaluateGradient, true &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Reflect the existing <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradientConst_3_01FunctionType_00_01Ha2a2b96a2e864e9694c577ab3c86846da.html#a0c8fa30ad928c3c0e5b5f535b15b763d">EvaluateWithGradient()</a>.  <a href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradientConst_3_01FunctionType_00_01Ha2a2b96a2e864e9694c577ab3c86846da.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradientConst_3_01FunctionType_00_01true_00_01false_01_4.html">AddDecomposableEvaluateWithGradientConst&lt; FunctionType, true, false &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">If we have both a decomposable const Evaluate() and a decomposable const Gradient() but not a decomposable const <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradientConst_3_01FunctionType_00_01true_00_01false_01_4.html#a0c8fa30ad928c3c0e5b5f535b15b763d" title="Return both the evaluated objective function and its gradient, storing the gradient in the given matr...">EvaluateWithGradient()</a>, add a decomposable const <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradientConst_3_01FunctionType_00_01true_00_01false_01_4.html#a0c8fa30ad928c3c0e5b5f535b15b763d" title="Return both the evaluated objective function and its gradient, storing the gradient in the given matr...">EvaluateWithGradient()</a> method.  <a href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradientConst_3_01FunctionType_00_01true_00_01false_01_4.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradientStatic.html">AddDecomposableEvaluateWithGradientStatic</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">The <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradientStatic.html" title="The AddDecomposableEvaluateWithGradientStatic mixin class will add a decomposable static EvaluateWith...">AddDecomposableEvaluateWithGradientStatic</a> mixin class will add a decomposable static <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradientStatic.html#a83d9060e7a957ce8f17b4a922352fdd8">EvaluateWithGradient()</a> method if both a decomposable static Evaluate() and a decomposable static gradient() function exist, or nothing otherwise.  <a href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradientStatic.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradientStatic_3_01FunctionType_00_01H4935bf7281d1b245d3bf096ee47eb9ca.html">AddDecomposableEvaluateWithGradientStatic&lt; FunctionType, HasDecomposableEvaluateGradient, true &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Reflect the existing <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradientStatic_3_01FunctionType_00_01H4935bf7281d1b245d3bf096ee47eb9ca.html#a6d792ca8e6e14151118d7081cbdcb817">EvaluateWithGradient()</a>.  <a href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradientStatic_3_01FunctionType_00_01H4935bf7281d1b245d3bf096ee47eb9ca.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradientStatic_3_01FunctionType_00_01true_00_01false_01_4.html">AddDecomposableEvaluateWithGradientStatic&lt; FunctionType, true, false &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">If we have a decomposable static Evaluate() and a decomposable static Gradient() but not a decomposable static <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradientStatic_3_01FunctionType_00_01true_00_01false_01_4.html#a0c8fa30ad928c3c0e5b5f535b15b763d" title="Return both the evaluated objective function and its gradient, storing the gradient in the given matr...">EvaluateWithGradient()</a>, add a decomposable static Gradient() method.  <a href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateWithGradientStatic_3_01FunctionType_00_01true_00_01false_01_4.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableGradient.html">AddDecomposableGradient</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">The <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableGradient.html" title="The AddDecomposableGradient mixin class will add a decomposable Gradient() method if a decomposable E...">AddDecomposableGradient</a> mixin class will add a decomposable <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableGradient.html#a35b4020f0b010c15eb49810e269e2c7a">Gradient()</a> method if a decomposable EvaluateWithGradient() function exists, or nothing otherwise.  <a href="classmlpack_1_1optimization_1_1AddDecomposableGradient.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableGradient_3_01FunctionType_00_01HasDecomposableEvaluateWithGradient_00_01true_01_4.html">AddDecomposableGradient&lt; FunctionType, HasDecomposableEvaluateWithGradient, true &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Reflect the existing <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableGradient_3_01FunctionType_00_01HasDecomposableEvaluateWithGradient_00_01true_01_4.html#a5fa8c51329cc29fc2ee7db1b4a3492c6">Gradient()</a>.  <a href="classmlpack_1_1optimization_1_1AddDecomposableGradient_3_01FunctionType_00_01HasDecomposableEvaluateWithGradient_00_01true_01_4.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableGradient_3_01FunctionType_00_01true_00_01false_01_4.html">AddDecomposableGradient&lt; FunctionType, true, false &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">If we have a decomposable EvaluateWithGradient() but not a decomposable <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableGradient_3_01FunctionType_00_01true_00_01false_01_4.html#a5fa8c51329cc29fc2ee7db1b4a3492c6" title="Calculate the gradient and store it in the given matrix, starting at the given decomposable function ...">Gradient()</a>, add a decomposable Evaluate() method.  <a href="classmlpack_1_1optimization_1_1AddDecomposableGradient_3_01FunctionType_00_01true_00_01false_01_4.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableGradientConst.html">AddDecomposableGradientConst</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">The <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableGradientConst.html" title="The AddDecomposableGradientConst mixin class will add a decomposable const Gradient() method if a dec...">AddDecomposableGradientConst</a> mixin class will add a decomposable const <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableGradientConst.html#af19a9b24e07a50aece3fe201673d54be">Gradient()</a> method if a decomposable const EvaluateWithGradient() function exists, or nothing otherwise.  <a href="classmlpack_1_1optimization_1_1AddDecomposableGradientConst.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableGradientConst_3_01FunctionType_00_01HasDecomposabld941c35c0c2712663880eea944f907f1.html">AddDecomposableGradientConst&lt; FunctionType, HasDecomposableEvaluateWithGradient, true &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Reflect the existing <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableGradientConst_3_01FunctionType_00_01HasDecomposabld941c35c0c2712663880eea944f907f1.html#a2101a0db624e44e0ed0f3da767997a4f">Gradient()</a>.  <a href="classmlpack_1_1optimization_1_1AddDecomposableGradientConst_3_01FunctionType_00_01HasDecomposabld941c35c0c2712663880eea944f907f1.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableGradientConst_3_01FunctionType_00_01true_00_01false_01_4.html">AddDecomposableGradientConst&lt; FunctionType, true, false &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">If we have a decomposable const EvaluateWithGradient() but not a decomposable const <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableGradientConst_3_01FunctionType_00_01true_00_01false_01_4.html#a2101a0db624e44e0ed0f3da767997a4f" title="Calculate the gradient and store it in the given matrix, starting at the given decomposable function ...">Gradient()</a>, add a decomposable const <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableGradientConst_3_01FunctionType_00_01true_00_01false_01_4.html#a2101a0db624e44e0ed0f3da767997a4f" title="Calculate the gradient and store it in the given matrix, starting at the given decomposable function ...">Gradient()</a> method.  <a href="classmlpack_1_1optimization_1_1AddDecomposableGradientConst_3_01FunctionType_00_01true_00_01false_01_4.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableGradientStatic.html">AddDecomposableGradientStatic</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">The <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableEvaluateStatic.html" title="The AddDecomposableEvaluateStatic mixin class will add a decomposable static Evaluate() method if a d...">AddDecomposableEvaluateStatic</a> mixin class will add a decomposable static <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableGradientStatic.html#af88e32fb61ae67ba759f2614333e6530">Gradient()</a> method if a decomposable static EvaluateWithGradient() function exists, or nothing otherwise.  <a href="classmlpack_1_1optimization_1_1AddDecomposableGradientStatic.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableGradientStatic_3_01FunctionType_00_01HasDecomposab8b875cfc929e96305d282fb1b8ec912c.html">AddDecomposableGradientStatic&lt; FunctionType, HasDecomposableEvaluateWithGradient, true &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Reflect the existing <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableGradientStatic_3_01FunctionType_00_01HasDecomposab8b875cfc929e96305d282fb1b8ec912c.html#abf1d5093aa3e33f771805a614f774875">Gradient()</a>.  <a href="classmlpack_1_1optimization_1_1AddDecomposableGradientStatic_3_01FunctionType_00_01HasDecomposab8b875cfc929e96305d282fb1b8ec912c.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableGradientStatic_3_01FunctionType_00_01true_00_01false_01_4.html">AddDecomposableGradientStatic&lt; FunctionType, true, false &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">If we have a decomposable EvaluateWithGradient() but not a decomposable <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableGradientStatic_3_01FunctionType_00_01true_00_01false_01_4.html#abf1d5093aa3e33f771805a614f774875" title="Calculate the gradient and store it in the given matrix, starting at the given decomposable function ...">Gradient()</a>, add a decomposable <a class="el" href="classmlpack_1_1optimization_1_1AddDecomposableGradientStatic_3_01FunctionType_00_01true_00_01false_01_4.html#abf1d5093aa3e33f771805a614f774875" title="Calculate the gradient and store it in the given matrix, starting at the given decomposable function ...">Gradient()</a> method.  <a href="classmlpack_1_1optimization_1_1AddDecomposableGradientStatic_3_01FunctionType_00_01true_00_01false_01_4.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddEvaluate.html">AddEvaluate</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">The <a class="el" href="classmlpack_1_1optimization_1_1AddEvaluate.html" title="The AddEvaluate mixin class will provide an Evaluate() method if the given FunctionType has EvaluateW...">AddEvaluate</a> mixin class will provide an <a class="el" href="classmlpack_1_1optimization_1_1AddEvaluate.html#aefb10620e2a9b060998e956b40a8db9e">Evaluate()</a> method if the given FunctionType has EvaluateWithGradient(), or nothing otherwise.  <a href="classmlpack_1_1optimization_1_1AddEvaluate.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddEvaluate_3_01FunctionType_00_01HasEvaluateWithGradient_00_01true_01_4.html">AddEvaluate&lt; FunctionType, HasEvaluateWithGradient, true &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Reflect the existing <a class="el" href="classmlpack_1_1optimization_1_1AddEvaluate_3_01FunctionType_00_01HasEvaluateWithGradient_00_01true_01_4.html#accb97ce3bd3a572c7a00dd5b9a1b89f6">Evaluate()</a>.  <a href="classmlpack_1_1optimization_1_1AddEvaluate_3_01FunctionType_00_01HasEvaluateWithGradient_00_01true_01_4.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddEvaluate_3_01FunctionType_00_01true_00_01false_01_4.html">AddEvaluate&lt; FunctionType, true, false &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">If we have EvaluateWithGradient() but no existing <a class="el" href="classmlpack_1_1optimization_1_1AddEvaluate_3_01FunctionType_00_01true_00_01false_01_4.html#accb97ce3bd3a572c7a00dd5b9a1b89f6" title="Return the objective function for the given coordinates. ">Evaluate()</a>, add an <a class="el" href="classmlpack_1_1optimization_1_1AddEvaluate_3_01FunctionType_00_01true_00_01false_01_4.html#accb97ce3bd3a572c7a00dd5b9a1b89f6" title="Return the objective function for the given coordinates. ">Evaluate()</a> method.  <a href="classmlpack_1_1optimization_1_1AddEvaluate_3_01FunctionType_00_01true_00_01false_01_4.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateConst.html">AddEvaluateConst</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">The <a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateConst.html" title="The AddEvaluateConst mixin class will provide a const Evaluate() method if the given FunctionType has...">AddEvaluateConst</a> mixin class will provide a const <a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateConst.html#addb8e37925edcca2a92c47eebeeb8b8d">Evaluate()</a> method if the given FunctionType has EvaluateWithGradient() const, or nothing otherwise.  <a href="classmlpack_1_1optimization_1_1AddEvaluateConst.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateConst_3_01FunctionType_00_01HasEvaluateWithGradient_00_01true_01_4.html">AddEvaluateConst&lt; FunctionType, HasEvaluateWithGradient, true &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Reflect the existing <a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateConst_3_01FunctionType_00_01HasEvaluateWithGradient_00_01true_01_4.html#a590d9517b5338e82029ee36597d04a9e">Evaluate()</a>.  <a href="classmlpack_1_1optimization_1_1AddEvaluateConst_3_01FunctionType_00_01HasEvaluateWithGradient_00_01true_01_4.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateConst_3_01FunctionType_00_01true_00_01false_01_4.html">AddEvaluateConst&lt; FunctionType, true, false &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">If we have EvaluateWithGradient() but no existing <a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateConst_3_01FunctionType_00_01true_00_01false_01_4.html#a590d9517b5338e82029ee36597d04a9e" title="Return the objective function for the given coordinates. ">Evaluate()</a>, add an <a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateConst_3_01FunctionType_00_01true_00_01false_01_4.html#a590d9517b5338e82029ee36597d04a9e" title="Return the objective function for the given coordinates. ">Evaluate()</a> without a using directive to make the base <a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateConst_3_01FunctionType_00_01true_00_01false_01_4.html#a590d9517b5338e82029ee36597d04a9e" title="Return the objective function for the given coordinates. ">Evaluate()</a> accessible.  <a href="classmlpack_1_1optimization_1_1AddEvaluateConst_3_01FunctionType_00_01true_00_01false_01_4.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateStatic.html">AddEvaluateStatic</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">The <a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateStatic.html" title="The AddEvaluateStatic mixin class will provide a static Evaluate() method if the given FunctionType h...">AddEvaluateStatic</a> mixin class will provide a static <a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateStatic.html#a8d1083f36e798521f3a479ec62fc8f48">Evaluate()</a> method if the given FunctionType has EvaluateWithGradient() static, or nothing otherwise.  <a href="classmlpack_1_1optimization_1_1AddEvaluateStatic.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateStatic_3_01FunctionType_00_01HasEvaluateWithGradient_00_01true_01_4.html">AddEvaluateStatic&lt; FunctionType, HasEvaluateWithGradient, true &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Reflect the existing <a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateStatic_3_01FunctionType_00_01HasEvaluateWithGradient_00_01true_01_4.html#ae52234a0c1e1620439a6381148f42b1e">Evaluate()</a>.  <a href="classmlpack_1_1optimization_1_1AddEvaluateStatic_3_01FunctionType_00_01HasEvaluateWithGradient_00_01true_01_4.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateStatic_3_01FunctionType_00_01true_00_01false_01_4.html">AddEvaluateStatic&lt; FunctionType, true, false &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">If we have EvaluateWithGradient() but no existing <a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateStatic_3_01FunctionType_00_01true_00_01false_01_4.html#ae52234a0c1e1620439a6381148f42b1e" title="Return the objective function for the given coordinates. ">Evaluate()</a>, add an <a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateStatic_3_01FunctionType_00_01true_00_01false_01_4.html#ae52234a0c1e1620439a6381148f42b1e" title="Return the objective function for the given coordinates. ">Evaluate()</a> without a using directive to make the base <a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateStatic_3_01FunctionType_00_01true_00_01false_01_4.html#ae52234a0c1e1620439a6381148f42b1e" title="Return the objective function for the given coordinates. ">Evaluate()</a> accessible.  <a href="classmlpack_1_1optimization_1_1AddEvaluateStatic_3_01FunctionType_00_01true_00_01false_01_4.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateWithGradient.html">AddEvaluateWithGradient</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">The <a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateWithGradient.html" title="The AddEvaluateWithGradient mixin class will provide an EvaluateWithGradient() method if the given Fu...">AddEvaluateWithGradient</a> mixin class will provide an <a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateWithGradient.html#a6e4b66f14de06dbed4277c8362a220b9">EvaluateWithGradient()</a> method if the given FunctionType has both Evaluate() and Gradient(), or it will provide nothing otherwise.  <a href="classmlpack_1_1optimization_1_1AddEvaluateWithGradient.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateWithGradient_3_01FunctionType_00_01HasEvaluateGradient_00_01true_01_4.html">AddEvaluateWithGradient&lt; FunctionType, HasEvaluateGradient, true &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Reflect the existing <a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateWithGradient_3_01FunctionType_00_01HasEvaluateGradient_00_01true_01_4.html#a84a35c02e61e6affb74cadce9b0ea646">EvaluateWithGradient()</a>.  <a href="classmlpack_1_1optimization_1_1AddEvaluateWithGradient_3_01FunctionType_00_01HasEvaluateGradient_00_01true_01_4.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateWithGradient_3_01FunctionType_00_01true_00_01false_01_4.html">AddEvaluateWithGradient&lt; FunctionType, true, false &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">If the FunctionType has Evaluate() and Gradient(), provide <a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateWithGradient_3_01FunctionType_00_01true_00_01false_01_4.html#a84a35c02e61e6affb74cadce9b0ea646" title="Return both the evaluated objective function and its gradient, storing the gradient in the given matr...">EvaluateWithGradient()</a>.  <a href="classmlpack_1_1optimization_1_1AddEvaluateWithGradient_3_01FunctionType_00_01true_00_01false_01_4.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateWithGradientConst.html">AddEvaluateWithGradientConst</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">The <a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateWithGradient.html" title="The AddEvaluateWithGradient mixin class will provide an EvaluateWithGradient() method if the given Fu...">AddEvaluateWithGradient</a> mixin class will provide an EvaluateWithGradient() const method if the given FunctionType has both Evaluate() const and Gradient() const, or it will provide nothing otherwise.  <a href="classmlpack_1_1optimization_1_1AddEvaluateWithGradientConst.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateWithGradientConst_3_01FunctionType_00_01HasEvaluateGradient_00_01true_01_4.html">AddEvaluateWithGradientConst&lt; FunctionType, HasEvaluateGradient, true &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Reflect the existing <a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateWithGradientConst_3_01FunctionType_00_01HasEvaluateGradient_00_01true_01_4.html#a4d5743de3cb045a26f36e4a7dbdecb2a">EvaluateWithGradient()</a>.  <a href="classmlpack_1_1optimization_1_1AddEvaluateWithGradientConst_3_01FunctionType_00_01HasEvaluateGradient_00_01true_01_4.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateWithGradientConst_3_01FunctionType_00_01true_00_01false_01_4.html">AddEvaluateWithGradientConst&lt; FunctionType, true, false &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">If the FunctionType has Evaluate() const and Gradient() const, provide EvaluateWithGradient() const.  <a href="classmlpack_1_1optimization_1_1AddEvaluateWithGradientConst_3_01FunctionType_00_01true_00_01false_01_4.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateWithGradientStatic.html">AddEvaluateWithGradientStatic</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">The <a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateWithGradientStatic.html" title="The AddEvaluateWithGradientStatic mixin class will provide a static EvaluateWithGradient() method if ...">AddEvaluateWithGradientStatic</a> mixin class will provide a static <a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateWithGradientStatic.html#a85ab974f89e61e95f33e92f8967bdc1e">EvaluateWithGradient()</a> method if the given FunctionType has both static Evaluate() and static Gradient(), or it will provide nothing otherwise.  <a href="classmlpack_1_1optimization_1_1AddEvaluateWithGradientStatic.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateWithGradientStatic_3_01FunctionType_00_01HasEvaluateGradient_00_01true_01_4.html">AddEvaluateWithGradientStatic&lt; FunctionType, HasEvaluateGradient, true &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Reflect the existing <a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateWithGradientStatic_3_01FunctionType_00_01HasEvaluateGradient_00_01true_01_4.html#a3c127d36c16993b99309eaeb671709a8">EvaluateWithGradient()</a>.  <a href="classmlpack_1_1optimization_1_1AddEvaluateWithGradientStatic_3_01FunctionType_00_01HasEvaluateGradient_00_01true_01_4.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateWithGradientStatic_3_01FunctionType_00_01true_00_01false_01_4.html">AddEvaluateWithGradientStatic&lt; FunctionType, true, false &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">If the FunctionType has static Evaluate() and static Gradient(), provide static <a class="el" href="classmlpack_1_1optimization_1_1AddEvaluateWithGradientStatic_3_01FunctionType_00_01true_00_01false_01_4.html#a3c127d36c16993b99309eaeb671709a8" title="Return both the evaluated objective function and its gradient, storing the gradient in the given matr...">EvaluateWithGradient()</a>.  <a href="classmlpack_1_1optimization_1_1AddEvaluateWithGradientStatic_3_01FunctionType_00_01true_00_01false_01_4.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddGradient.html">AddGradient</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">The <a class="el" href="classmlpack_1_1optimization_1_1AddGradient.html" title="The AddGradient mixin class will provide a Gradient() method if the given FunctionType has EvaluateWi...">AddGradient</a> mixin class will provide a <a class="el" href="classmlpack_1_1optimization_1_1AddGradient.html#a32d3eb55b2e0b2c95749a204dc45c4ad">Gradient()</a> method if the given FunctionType has EvaluateWithGradient(), or nothing otherwise.  <a href="classmlpack_1_1optimization_1_1AddGradient.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddGradient_3_01FunctionType_00_01HasEvaluateWithGradient_00_01true_01_4.html">AddGradient&lt; FunctionType, HasEvaluateWithGradient, true &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Reflect the existing <a class="el" href="classmlpack_1_1optimization_1_1AddGradient_3_01FunctionType_00_01HasEvaluateWithGradient_00_01true_01_4.html#a11434747bc8e23dc5f4c8e66c06265ea">Gradient()</a>.  <a href="classmlpack_1_1optimization_1_1AddGradient_3_01FunctionType_00_01HasEvaluateWithGradient_00_01true_01_4.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddGradient_3_01FunctionType_00_01true_00_01false_01_4.html">AddGradient&lt; FunctionType, true, false &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">If we have EvaluateWithGradient() but no existing <a class="el" href="classmlpack_1_1optimization_1_1AddGradient_3_01FunctionType_00_01true_00_01false_01_4.html#a11434747bc8e23dc5f4c8e66c06265ea" title="Calculate the gradient and store it in the given matrix. ">Gradient()</a>, add an <a class="el" href="classmlpack_1_1optimization_1_1AddGradient_3_01FunctionType_00_01true_00_01false_01_4.html#a11434747bc8e23dc5f4c8e66c06265ea" title="Calculate the gradient and store it in the given matrix. ">Gradient()</a> without a using directive to make the base <a class="el" href="classmlpack_1_1optimization_1_1AddGradient_3_01FunctionType_00_01true_00_01false_01_4.html#a11434747bc8e23dc5f4c8e66c06265ea" title="Calculate the gradient and store it in the given matrix. ">Gradient()</a> accessible.  <a href="classmlpack_1_1optimization_1_1AddGradient_3_01FunctionType_00_01true_00_01false_01_4.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddGradientConst.html">AddGradientConst</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">The <a class="el" href="classmlpack_1_1optimization_1_1AddGradient.html" title="The AddGradient mixin class will provide a Gradient() method if the given FunctionType has EvaluateWi...">AddGradient</a> mixin class will provide a const <a class="el" href="classmlpack_1_1optimization_1_1AddGradientConst.html#af271debf0975d43e643abb16f91e7ca8">Gradient()</a> method if the given FunctionType has EvaluateWithGradient() const, or nothing otherwise.  <a href="classmlpack_1_1optimization_1_1AddGradientConst.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddGradientConst_3_01FunctionType_00_01HasEvaluateWithGradient_00_01true_01_4.html">AddGradientConst&lt; FunctionType, HasEvaluateWithGradient, true &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Reflect the existing <a class="el" href="classmlpack_1_1optimization_1_1AddGradientConst_3_01FunctionType_00_01HasEvaluateWithGradient_00_01true_01_4.html#a10d32cb3f4599281f55ccfb20cfbc4be">Gradient()</a>.  <a href="classmlpack_1_1optimization_1_1AddGradientConst_3_01FunctionType_00_01HasEvaluateWithGradient_00_01true_01_4.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddGradientConst_3_01FunctionType_00_01true_00_01false_01_4.html">AddGradientConst&lt; FunctionType, true, false &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">If we have EvaluateWithGradient() but no existing <a class="el" href="classmlpack_1_1optimization_1_1AddGradientConst_3_01FunctionType_00_01true_00_01false_01_4.html#a10d32cb3f4599281f55ccfb20cfbc4be" title="Calculate the gradient and store it in the given matrix. ">Gradient()</a>, add a <a class="el" href="classmlpack_1_1optimization_1_1AddGradientConst_3_01FunctionType_00_01true_00_01false_01_4.html#a10d32cb3f4599281f55ccfb20cfbc4be" title="Calculate the gradient and store it in the given matrix. ">Gradient()</a> without a using directive to make the base <a class="el" href="classmlpack_1_1optimization_1_1AddGradientConst_3_01FunctionType_00_01true_00_01false_01_4.html#a10d32cb3f4599281f55ccfb20cfbc4be" title="Calculate the gradient and store it in the given matrix. ">Gradient()</a> accessible.  <a href="classmlpack_1_1optimization_1_1AddGradientConst_3_01FunctionType_00_01true_00_01false_01_4.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddGradientStatic.html">AddGradientStatic</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">The <a class="el" href="classmlpack_1_1optimization_1_1AddGradient.html" title="The AddGradient mixin class will provide a Gradient() method if the given FunctionType has EvaluateWi...">AddGradient</a> mixin class will provide a static <a class="el" href="classmlpack_1_1optimization_1_1AddGradientStatic.html#ad18098ad3f4c5eb53c45639f5c2ae188">Gradient()</a> method if the given FunctionType has static EvaluateWithGradient(), or nothing otherwise.  <a href="classmlpack_1_1optimization_1_1AddGradientStatic.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddGradientStatic_3_01FunctionType_00_01HasEvaluateWithGradient_00_01true_01_4.html">AddGradientStatic&lt; FunctionType, HasEvaluateWithGradient, true &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Reflect the existing <a class="el" href="classmlpack_1_1optimization_1_1AddGradientStatic_3_01FunctionType_00_01HasEvaluateWithGradient_00_01true_01_4.html#ad9fcda17dc2f2bce2ffa0113775ca620">Gradient()</a>.  <a href="classmlpack_1_1optimization_1_1AddGradientStatic_3_01FunctionType_00_01HasEvaluateWithGradient_00_01true_01_4.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AddGradientStatic_3_01FunctionType_00_01true_00_01false_01_4.html">AddGradientStatic&lt; FunctionType, true, false &gt;</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">If we have EvaluateWithGradient() but no existing <a class="el" href="classmlpack_1_1optimization_1_1AddGradientStatic_3_01FunctionType_00_01true_00_01false_01_4.html#ad9fcda17dc2f2bce2ffa0113775ca620" title="Calculate the gradient and store it in the given matrix. ">Gradient()</a>, add a <a class="el" href="classmlpack_1_1optimization_1_1AddGradientStatic_3_01FunctionType_00_01true_00_01false_01_4.html#ad9fcda17dc2f2bce2ffa0113775ca620" title="Calculate the gradient and store it in the given matrix. ">Gradient()</a> without a using directive to make the base <a class="el" href="classmlpack_1_1optimization_1_1AddGradientStatic_3_01FunctionType_00_01true_00_01false_01_4.html#ad9fcda17dc2f2bce2ffa0113775ca620" title="Calculate the gradient and store it in the given matrix. ">Gradient()</a> accessible.  <a href="classmlpack_1_1optimization_1_1AddGradientStatic_3_01FunctionType_00_01true_00_01false_01_4.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AMSGradUpdate.html">AMSGradUpdate</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">AMSGrad is an exponential moving average variant which along with having benefits of optimizers like Adam and <a class="el" href="classmlpack_1_1optimization_1_1RMSProp.html" title="RMSProp is an optimizer that utilizes the magnitude of recent gradients to normalize the gradients...">RMSProp</a>, also guarantees convergence.  <a href="classmlpack_1_1optimization_1_1AMSGradUpdate.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1Atoms.html">Atoms</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Class to hold the information and operations of current atoms in the soluton space.  <a href="classmlpack_1_1optimization_1_1Atoms.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AugLagrangian.html">AugLagrangian</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">The <a class="el" href="classmlpack_1_1optimization_1_1AugLagrangian.html" title="The AugLagrangian class implements the Augmented Lagrangian method of optimization. ">AugLagrangian</a> class implements the Augmented Lagrangian method of optimization.  <a href="classmlpack_1_1optimization_1_1AugLagrangian.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AugLagrangianFunction.html">AugLagrangianFunction</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">This is a utility class used by <a class="el" href="classmlpack_1_1optimization_1_1AugLagrangian.html" title="The AugLagrangian class implements the Augmented Lagrangian method of optimization. ">AugLagrangian</a>, meant to wrap a LagrangianFunction into a function usable by a simple optimizer like L-BFGS.  <a href="classmlpack_1_1optimization_1_1AugLagrangianFunction.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AugLagrangianTestFunction.html">AugLagrangianTestFunction</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">This function is taken from "Practical Mathematical Optimization" (Snyman), section 5.3.8 ("Application of the Augmented Lagrangian Method").  <a href="classmlpack_1_1optimization_1_1AugLagrangianTestFunction.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1BacktrackingLineSearch.html">BacktrackingLineSearch</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Definition of the backtracking line search algorithm based on the Armijo–Goldstein condition to determine the maximum amount to move along the given search direction.  <a href="classmlpack_1_1optimization_1_1BacktrackingLineSearch.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1BarzilaiBorweinDecay.html">BarzilaiBorweinDecay</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Barzilai-Borwein decay policy for Stochastic variance reduced gradient (SVRG).  <a href="classmlpack_1_1optimization_1_1BarzilaiBorweinDecay.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1BigBatchSGD.html">BigBatchSGD</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Big-batch Stochastic Gradient Descent is a technique for minimizing a function which can be expressed as a sum of other functions.  <a href="classmlpack_1_1optimization_1_1BigBatchSGD.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1CMAES.html">CMAES</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">CMA-ES - Covariance Matrix Adaptation Evolution Strategy is s a stochastic search algorithm.  <a href="classmlpack_1_1optimization_1_1CMAES.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1CNE.html">CNE</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Conventional Neural Evolution (<a class="el" href="classmlpack_1_1optimization_1_1CNE.html" title="Conventional Neural Evolution (CNE) is a class of evolutionary algorithms focused on dealing with fix...">CNE</a>) is a class of evolutionary algorithms focused on dealing with fixed topology.  <a href="classmlpack_1_1optimization_1_1CNE.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1ConstantStep.html">ConstantStep</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Implementation of the <a class="el" href="classmlpack_1_1optimization_1_1ConstantStep.html" title="Implementation of the ConstantStep stepsize decay policy for parallel SGD. ">ConstantStep</a> stepsize decay policy for parallel <a class="el" href="classmlpack_1_1optimization_1_1SGD.html" title="Stochastic Gradient Descent is a technique for minimizing a function which can be expressed as a sum ...">SGD</a>.  <a href="classmlpack_1_1optimization_1_1ConstantStep.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1ConstrLpBallSolver.html">ConstrLpBallSolver</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">LinearConstrSolver for <a class="el" href="classmlpack_1_1optimization_1_1FrankWolfe.html" title="Frank-Wolfe is a technique to minimize a continuously differentiable convex function  over a compact ...">FrankWolfe</a> algorithm.  <a href="classmlpack_1_1optimization_1_1ConstrLpBallSolver.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1ConstrStructGroupSolver.html">ConstrStructGroupSolver</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Linear Constrained Solver for <a class="el" href="classmlpack_1_1optimization_1_1FrankWolfe.html" title="Frank-Wolfe is a technique to minimize a continuously differentiable convex function  over a compact ...">FrankWolfe</a>.  <a href="classmlpack_1_1optimization_1_1ConstrStructGroupSolver.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1CyclicalDecay.html">CyclicalDecay</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Simulate a new warm-started run/restart once a number of epochs are performed.  <a href="classmlpack_1_1optimization_1_1CyclicalDecay.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1CyclicDescent.html">CyclicDescent</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Cyclic descent policy for Stochastic Coordinate Descent(SCD).  <a href="classmlpack_1_1optimization_1_1CyclicDescent.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1ExponentialBackoff.html">ExponentialBackoff</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Exponential backoff stepsize reduction policy for parallel <a class="el" href="classmlpack_1_1optimization_1_1SGD.html" title="Stochastic Gradient Descent is a technique for minimizing a function which can be expressed as a sum ...">SGD</a>.  <a href="classmlpack_1_1optimization_1_1ExponentialBackoff.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1ExponentialSchedule.html">ExponentialSchedule</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">The exponential cooling schedule cools the temperature T at every step according to the equation.  <a href="classmlpack_1_1optimization_1_1ExponentialSchedule.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1FrankWolfe.html">FrankWolfe</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Frank-Wolfe is a technique to minimize a continuously differentiable convex function <img class="formulaInl" alt="$ f $" src="form_77.png"></img> over a compact convex subset <img class="formulaInl" alt="$ D $" src="form_62.png"></img> of a vector space.  <a href="classmlpack_1_1optimization_1_1FrankWolfe.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1FullSelection.html">FullSelection</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1FuncSq.html">FuncSq</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Square loss function <img class="formulaInl" alt="$ f(x) = 0.5 * ||Ax - b||_2^2 $" src="form_87.png"></img>.  <a href="classmlpack_1_1optimization_1_1FuncSq.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1Function.html">Function</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">The <a class="el" href="classmlpack_1_1optimization_1_1Function.html" title="The Function class is a wrapper class for any FunctionType that will add any possible derived methods...">Function</a> class is a wrapper class for any FunctionType that will add any possible derived methods.  <a href="classmlpack_1_1optimization_1_1Function.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1GockenbachFunction.html">GockenbachFunction</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">This function is taken from M.  <a href="classmlpack_1_1optimization_1_1GockenbachFunction.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1GradientClipping.html">GradientClipping</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Interface for wrapping around update policies (e.g., <a class="el" href="classmlpack_1_1optimization_1_1VanillaUpdate.html" title="Vanilla update policy for Stochastic Gradient Descent (SGD). ">VanillaUpdate</a>) and feeding a clipped gradient to them instead of the normal one.  <a href="classmlpack_1_1optimization_1_1GradientClipping.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1GradientDescent.html">GradientDescent</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Gradient Descent is a technique to minimize a function.  <a href="classmlpack_1_1optimization_1_1GradientDescent.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1GreedyDescent.html">GreedyDescent</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Greedy descent policy for Stochastic Co-ordinate Descent(SCD).  <a href="classmlpack_1_1optimization_1_1GreedyDescent.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1GridSearch.html">GridSearch</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">An optimizer that finds the minimum of a given function by iterating through points on a multidimensional grid.  <a href="classmlpack_1_1optimization_1_1GridSearch.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1GroupLpBall.html">GroupLpBall</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Implementation of Structured Group.  <a href="classmlpack_1_1optimization_1_1GroupLpBall.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1IQN.html">IQN</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight"><a class="el" href="classmlpack_1_1optimization_1_1IQN.html" title="IQN is a technique for minimizing a function which can be expressed as a sum of other functions...">IQN</a> is a technique for minimizing a function which can be expressed as a sum of other functions.  <a href="classmlpack_1_1optimization_1_1IQN.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1KatyushaType.html">KatyushaType</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Katyusha is a direct, primal-only stochastic gradient method which uses a "negative momentum" on top of Nesterov’s momentum.  <a href="classmlpack_1_1optimization_1_1KatyushaType.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1L__BFGS.html">L_BFGS</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">The generic L-BFGS optimizer, which uses a back-tracking line search algorithm to minimize a function.  <a href="classmlpack_1_1optimization_1_1L__BFGS.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1LineSearch.html">LineSearch</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Find the minimum of a function along the line between two points.  <a href="classmlpack_1_1optimization_1_1LineSearch.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1LovaszThetaSDP.html">LovaszThetaSDP</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">This function is the Lovasz-Theta semidefinite program, as implemented in the following paper:  <a href="classmlpack_1_1optimization_1_1LovaszThetaSDP.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1LRSDP.html">LRSDP</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight"><a class="el" href="classmlpack_1_1optimization_1_1LRSDP.html" title="LRSDP is the implementation of Monteiro and Burer&apos;s formulation of low-rank semidefinite programs (LR...">LRSDP</a> is the implementation of Monteiro and Burer's formulation of low-rank semidefinite programs (LR-SDP).  <a href="classmlpack_1_1optimization_1_1LRSDP.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1LRSDPFunction.html">LRSDPFunction</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">The objective function that <a class="el" href="classmlpack_1_1optimization_1_1LRSDP.html" title="LRSDP is the implementation of Monteiro and Burer&apos;s formulation of low-rank semidefinite programs (LR...">LRSDP</a> is trying to optimize.  <a href="classmlpack_1_1optimization_1_1LRSDPFunction.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1NadaMaxUpdate.html">NadaMaxUpdate</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">NadaMax is an optimizer that combines the AdaMax and NAG.  <a href="classmlpack_1_1optimization_1_1NadaMaxUpdate.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1NadamUpdate.html">NadamUpdate</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Nadam is an optimizer that combines the Adam and NAG optimization strategies.  <a href="classmlpack_1_1optimization_1_1NadamUpdate.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1NesterovMomentumUpdate.html">NesterovMomentumUpdate</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Nesterov Momentum update policy for Stochastic Gradient Descent (<a class="el" href="classmlpack_1_1optimization_1_1SGD.html" title="Stochastic Gradient Descent is a technique for minimizing a function which can be expressed as a sum ...">SGD</a>).  <a href="classmlpack_1_1optimization_1_1NesterovMomentumUpdate.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1NoDecay.html">NoDecay</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Definition of the <a class="el" href="classmlpack_1_1optimization_1_1NoDecay.html" title="Definition of the NoDecay class. ">NoDecay</a> class.  <a href="classmlpack_1_1optimization_1_1NoDecay.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1OptimisticAdamUpdate.html">OptimisticAdamUpdate</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">OptimisticAdam is an optimizer which implements the Optimistic Adam algorithm which uses Optmistic Mirror Descent with the Adam Optimizer.  <a href="classmlpack_1_1optimization_1_1OptimisticAdamUpdate.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1ParallelSGD.html">ParallelSGD</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">An implementation of parallel stochastic gradient descent using the lock-free HOGWILD! approach.  <a href="classmlpack_1_1optimization_1_1ParallelSGD.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1PrimalDualSolver.html">PrimalDualSolver</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Interface to a primal dual interior point solver.  <a href="classmlpack_1_1optimization_1_1PrimalDualSolver.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1Proximal.html">Proximal</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Approximate a vector with another vector on lp ball.  <a href="classmlpack_1_1optimization_1_1Proximal.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1RandomDescent.html">RandomDescent</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Random descent policy for Stochastic Coordinate Descent(SCD).  <a href="classmlpack_1_1optimization_1_1RandomDescent.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1RandomSelection.html">RandomSelection</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1RMSProp.html">RMSProp</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight"><a class="el" href="classmlpack_1_1optimization_1_1RMSProp.html" title="RMSProp is an optimizer that utilizes the magnitude of recent gradients to normalize the gradients...">RMSProp</a> is an optimizer that utilizes the magnitude of recent gradients to normalize the gradients.  <a href="classmlpack_1_1optimization_1_1RMSProp.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1RMSPropUpdate.html">RMSPropUpdate</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight"><a class="el" href="classmlpack_1_1optimization_1_1RMSProp.html" title="RMSProp is an optimizer that utilizes the magnitude of recent gradients to normalize the gradients...">RMSProp</a> is an optimizer that utilizes the magnitude of recent gradients to normalize the gradients.  <a href="classmlpack_1_1optimization_1_1RMSPropUpdate.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1SA.html">SA</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Simulated Annealing is an stochastic optimization algorithm which is able to deliver near-optimal results quickly without knowing the gradient of the function being optimized.  <a href="classmlpack_1_1optimization_1_1SA.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1SARAHPlusUpdate.html">SARAHPlusUpdate</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">SARAH+ provides an automatic and adaptive choice of the inner loop size.  <a href="classmlpack_1_1optimization_1_1SARAHPlusUpdate.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1SARAHType.html">SARAHType</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">StochAstic Recusive gRadient algoritHm (SARAH).  <a href="classmlpack_1_1optimization_1_1SARAHType.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1SARAHUpdate.html">SARAHUpdate</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Vanilla update policy for SARAH.  <a href="classmlpack_1_1optimization_1_1SARAHUpdate.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1SCD.html">SCD</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Stochastic Coordinate descent is a technique for minimizing a function by doing a line search along a single direction at the current point in the iteration.  <a href="classmlpack_1_1optimization_1_1SCD.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1SDP.html">SDP</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Specify an <a class="el" href="classmlpack_1_1optimization_1_1SDP.html" title="Specify an SDP in primal form. ">SDP</a> in primal form.  <a href="classmlpack_1_1optimization_1_1SDP.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1SGD.html">SGD</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Stochastic Gradient Descent is a technique for minimizing a function which can be expressed as a sum of other functions.  <a href="classmlpack_1_1optimization_1_1SGD.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1SGDR.html">SGDR</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">This class is based on Mini-batch Stochastic Gradient Descent class and simulates a new warm-started run/restart once a number of epochs are performed.  <a href="classmlpack_1_1optimization_1_1SGDR.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1SMORMS3.html">SMORMS3</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight"><a class="el" href="classmlpack_1_1optimization_1_1SMORMS3.html" title="SMORMS3 is an optimizer that estimates a safe and optimal distance based on curvature and normalizing...">SMORMS3</a> is an optimizer that estimates a safe and optimal distance based on curvature and normalizing the stepsize in the parameter space.  <a href="classmlpack_1_1optimization_1_1SMORMS3.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1SMORMS3Update.html">SMORMS3Update</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight"><a class="el" href="classmlpack_1_1optimization_1_1SMORMS3.html" title="SMORMS3 is an optimizer that estimates a safe and optimal distance based on curvature and normalizing...">SMORMS3</a> is an optimizer that estimates a safe and optimal distance based on curvature and normalizing the stepsize in the parameter space.  <a href="classmlpack_1_1optimization_1_1SMORMS3Update.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1SnapshotEnsembles.html">SnapshotEnsembles</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Simulate a new warm-started run/restart once a number of epochs are performed.  <a href="classmlpack_1_1optimization_1_1SnapshotEnsembles.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1SnapshotSGDR.html">SnapshotSGDR</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">This class is based on Mini-batch Stochastic Gradient Descent class and simulates a new warm-started run/restart once a number of epochs are performed using the Snapshot ensembles technique.  <a href="classmlpack_1_1optimization_1_1SnapshotSGDR.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1SPALeRASGD.html">SPALeRASGD</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">SPALeRA Stochastic Gradient Descent is a technique for minimizing a function which can be expressed as a sum of other functions.  <a href="classmlpack_1_1optimization_1_1SPALeRASGD.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1SPALeRAStepsize.html">SPALeRAStepsize</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Definition of the SPALeRA stepize technique, which implementes a change detection mechanism with an agnostic adaptation scheme.  <a href="classmlpack_1_1optimization_1_1SPALeRAStepsize.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1SVRGType.html">SVRGType</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Stochastic Variance Reduced Gradient is a technique for minimizing a function which can be expressed as a sum of other functions.  <a href="classmlpack_1_1optimization_1_1SVRGType.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1SVRGUpdate.html">SVRGUpdate</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Vanilla update policy for Stochastic variance reduced gradient (SVRG).  <a href="classmlpack_1_1optimization_1_1SVRGUpdate.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1TestFuncFW.html">TestFuncFW</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Simple test function for classic Frank Wolfe Algorithm:  <a href="classmlpack_1_1optimization_1_1TestFuncFW.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1UpdateClassic.html">UpdateClassic</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Use classic rule in the update step for <a class="el" href="classmlpack_1_1optimization_1_1FrankWolfe.html" title="Frank-Wolfe is a technique to minimize a continuously differentiable convex function  over a compact ...">FrankWolfe</a> algorithm.  <a href="classmlpack_1_1optimization_1_1UpdateClassic.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1UpdateFullCorrection.html">UpdateFullCorrection</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Full correction approach to update the solution.  <a href="classmlpack_1_1optimization_1_1UpdateFullCorrection.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1UpdateLineSearch.html">UpdateLineSearch</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Use line search in the update step for <a class="el" href="classmlpack_1_1optimization_1_1FrankWolfe.html" title="Frank-Wolfe is a technique to minimize a continuously differentiable convex function  over a compact ...">FrankWolfe</a> algorithm.  <a href="classmlpack_1_1optimization_1_1UpdateLineSearch.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1UpdateSpan.html">UpdateSpan</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Recalculate the optimal solution in the span of all previous solution space, used as update step for <a class="el" href="classmlpack_1_1optimization_1_1FrankWolfe.html" title="Frank-Wolfe is a technique to minimize a continuously differentiable convex function  over a compact ...">FrankWolfe</a> algorithm.  <a href="classmlpack_1_1optimization_1_1UpdateSpan.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1VanillaUpdate.html">VanillaUpdate</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Vanilla update policy for Stochastic Gradient Descent (<a class="el" href="classmlpack_1_1optimization_1_1SGD.html" title="Stochastic Gradient Descent is a technique for minimizing a function which can be expressed as a sum ...">SGD</a>).  <a href="classmlpack_1_1optimization_1_1VanillaUpdate.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="typedef-members"></a>
Typedefs</h2></td></tr>
<tr class="memitem:ae015766100b47037db7b997462facc97"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacemlpack_1_1optimization.html#ae015766100b47037db7b997462facc97">Adam</a> = <a class="el" href="classmlpack_1_1optimization_1_1AdamType.html">AdamType</a>&lt; <a class="el" href="classmlpack_1_1optimization_1_1AdamUpdate.html">AdamUpdate</a> &gt;</td></tr>
<tr class="separator:ae015766100b47037db7b997462facc97"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7d6274d40de7c01efc1ae34158a319cc"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacemlpack_1_1optimization.html#a7d6274d40de7c01efc1ae34158a319cc">AdaMax</a> = <a class="el" href="classmlpack_1_1optimization_1_1AdamType.html">AdamType</a>&lt; <a class="el" href="classmlpack_1_1optimization_1_1AdaMaxUpdate.html">AdaMaxUpdate</a> &gt;</td></tr>
<tr class="separator:a7d6274d40de7c01efc1ae34158a319cc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a07b6ef20451d6c19952d44ff6c52e3bb"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacemlpack_1_1optimization.html#a07b6ef20451d6c19952d44ff6c52e3bb">AMSGrad</a> = <a class="el" href="classmlpack_1_1optimization_1_1AdamType.html">AdamType</a>&lt; <a class="el" href="classmlpack_1_1optimization_1_1AMSGradUpdate.html">AMSGradUpdate</a> &gt;</td></tr>
<tr class="separator:a07b6ef20451d6c19952d44ff6c52e3bb"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4a29a6f28bda38f78a6068c898c7c20d"><td class="memTemplParams" colspan="2"><div class="template_expr"><div class="template_decl">template</div><div class="open_bracket">&lt;</div><div class="template_param_list"><div class="template_param"><div class="type_decl">typename</div><div class="identifier">SelectionPolicyType</div><div class="default_argument"><div class="equals">=</div><div class="identifier">RandomSelection</div></div><div class="close_bracket">&gt;</div></div></div></div></td></tr>
<tr class="memitem:a4a29a6f28bda38f78a6068c898c7c20d"><td class="memTemplItemLeft" align="right" valign="top">using&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespacemlpack_1_1optimization.html#a4a29a6f28bda38f78a6068c898c7c20d">ApproxCMAES</a> = <a class="el" href="classmlpack_1_1optimization_1_1CMAES.html">CMAES</a>&lt; SelectionPolicyType &gt;</td></tr>
<tr class="memdesc:a4a29a6f28bda38f78a6068c898c7c20d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Convenient typedef for <a class="el" href="classmlpack_1_1optimization_1_1CMAES.html" title="CMA-ES - Covariance Matrix Adaptation Evolution Strategy is s a stochastic search algorithm...">CMAES</a> approximation.  <a href="#a4a29a6f28bda38f78a6068c898c7c20d">More...</a><br /></br></td></tr>
<tr class="separator:a4a29a6f28bda38f78a6068c898c7c20d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a78364270bd0338acc1a577f7f2ec369b"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacemlpack_1_1optimization.html#a78364270bd0338acc1a577f7f2ec369b">BBS_Armijo</a> = <a class="el" href="classmlpack_1_1optimization_1_1BigBatchSGD.html">BigBatchSGD</a>&lt; <a class="el" href="classmlpack_1_1optimization_1_1BacktrackingLineSearch.html">BacktrackingLineSearch</a> &gt;</td></tr>
<tr class="separator:a78364270bd0338acc1a577f7f2ec369b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad46e76af14b52ffbc541df79fe51a024"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacemlpack_1_1optimization.html#ad46e76af14b52ffbc541df79fe51a024">BBS_BB</a> = <a class="el" href="classmlpack_1_1optimization_1_1BigBatchSGD.html">BigBatchSGD</a>&lt; <a class="el" href="classmlpack_1_1optimization_1_1AdaptiveStepsize.html">AdaptiveStepsize</a> &gt;</td></tr>
<tr class="separator:ad46e76af14b52ffbc541df79fe51a024"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a06530afc92ee02d33b08e8c32860bee6"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacemlpack_1_1optimization.html#a06530afc92ee02d33b08e8c32860bee6">Katyusha</a> = <a class="el" href="classmlpack_1_1optimization_1_1KatyushaType.html">KatyushaType</a>&lt; false &gt;</td></tr>
<tr class="memdesc:a06530afc92ee02d33b08e8c32860bee6"><td class="mdescLeft">&#160;</td><td class="mdescRight">Katyusha using the standard update step.  <a href="#a06530afc92ee02d33b08e8c32860bee6">More...</a><br /></br></td></tr>
<tr class="separator:a06530afc92ee02d33b08e8c32860bee6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6654a0baa3bab4b02d9b6e5eb901397f"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacemlpack_1_1optimization.html#a6654a0baa3bab4b02d9b6e5eb901397f">KatyushaProximal</a> = <a class="el" href="classmlpack_1_1optimization_1_1KatyushaType.html">KatyushaType</a>&lt; true &gt;</td></tr>
<tr class="memdesc:a6654a0baa3bab4b02d9b6e5eb901397f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Katyusha using the proximal update step.  <a href="#a6654a0baa3bab4b02d9b6e5eb901397f">More...</a><br /></br></td></tr>
<tr class="separator:a6654a0baa3bab4b02d9b6e5eb901397f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae5ae5626c8ca6228b90f6e11c02eb1f8"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacemlpack_1_1optimization.html#ae5ae5626c8ca6228b90f6e11c02eb1f8">MomentumSGD</a> = <a class="el" href="classmlpack_1_1optimization_1_1SGD.html">SGD</a>&lt; MomentumUpdate &gt;</td></tr>
<tr class="separator:ae5ae5626c8ca6228b90f6e11c02eb1f8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a373a947f96aee47a23b0f24a9760c2a4"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacemlpack_1_1optimization.html#a373a947f96aee47a23b0f24a9760c2a4">Nadam</a> = <a class="el" href="classmlpack_1_1optimization_1_1AdamType.html">AdamType</a>&lt; <a class="el" href="classmlpack_1_1optimization_1_1NadamUpdate.html">NadamUpdate</a> &gt;</td></tr>
<tr class="separator:a373a947f96aee47a23b0f24a9760c2a4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad8acb27595de9dc717961e6e9b0d89bd"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacemlpack_1_1optimization.html#ad8acb27595de9dc717961e6e9b0d89bd">NadaMax</a> = <a class="el" href="classmlpack_1_1optimization_1_1AdamType.html">AdamType</a>&lt; <a class="el" href="classmlpack_1_1optimization_1_1NadaMaxUpdate.html">NadaMaxUpdate</a> &gt;</td></tr>
<tr class="separator:ad8acb27595de9dc717961e6e9b0d89bd"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a92d67fc88582396a07a0ed76e72329c6"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacemlpack_1_1optimization.html#a92d67fc88582396a07a0ed76e72329c6">NesterovMomentumSGD</a> = <a class="el" href="classmlpack_1_1optimization_1_1SGD.html">SGD</a>&lt; <a class="el" href="classmlpack_1_1optimization_1_1NesterovMomentumUpdate.html">NesterovMomentumUpdate</a> &gt;</td></tr>
<tr class="separator:a92d67fc88582396a07a0ed76e72329c6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a54120a755fc1e801162e212657f6ceff"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacemlpack_1_1optimization.html#a54120a755fc1e801162e212657f6ceff">OMP</a> = <a class="el" href="classmlpack_1_1optimization_1_1FrankWolfe.html">FrankWolfe</a>&lt; <a class="el" href="classmlpack_1_1optimization_1_1ConstrLpBallSolver.html">ConstrLpBallSolver</a>, <a class="el" href="classmlpack_1_1optimization_1_1UpdateSpan.html">UpdateSpan</a> &gt;</td></tr>
<tr class="memdesc:a54120a755fc1e801162e212657f6ceff"><td class="mdescLeft">&#160;</td><td class="mdescRight">Orthogonal Matching Pursuit.  <a href="#a54120a755fc1e801162e212657f6ceff">More...</a><br /></br></td></tr>
<tr class="separator:a54120a755fc1e801162e212657f6ceff"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aae31eb5e7dce7f22edfe89018785e632"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacemlpack_1_1optimization.html#aae31eb5e7dce7f22edfe89018785e632">OptimisticAdam</a> = <a class="el" href="classmlpack_1_1optimization_1_1AdamType.html">AdamType</a>&lt; <a class="el" href="classmlpack_1_1optimization_1_1OptimisticAdamUpdate.html">OptimisticAdamUpdate</a> &gt;</td></tr>
<tr class="separator:aae31eb5e7dce7f22edfe89018785e632"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af3f51b1e39d7b8c88aef4c0a3cd31ddd"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacemlpack_1_1optimization.html#af3f51b1e39d7b8c88aef4c0a3cd31ddd">SARAH</a> = <a class="el" href="classmlpack_1_1optimization_1_1SARAHType.html">SARAHType</a>&lt; <a class="el" href="classmlpack_1_1optimization_1_1SARAHUpdate.html">SARAHUpdate</a> &gt;</td></tr>
<tr class="memdesc:af3f51b1e39d7b8c88aef4c0a3cd31ddd"><td class="mdescLeft">&#160;</td><td class="mdescRight">Standard stochastic variance reduced gradient.  <a href="#af3f51b1e39d7b8c88aef4c0a3cd31ddd">More...</a><br /></br></td></tr>
<tr class="separator:af3f51b1e39d7b8c88aef4c0a3cd31ddd"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae0bcb29037b15bffb25fb75d31e2e4a5"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacemlpack_1_1optimization.html#ae0bcb29037b15bffb25fb75d31e2e4a5">SARAH_Plus</a> = <a class="el" href="classmlpack_1_1optimization_1_1SARAHType.html">SARAHType</a>&lt; <a class="el" href="classmlpack_1_1optimization_1_1SARAHPlusUpdate.html">SARAHPlusUpdate</a> &gt;</td></tr>
<tr class="memdesc:ae0bcb29037b15bffb25fb75d31e2e4a5"><td class="mdescLeft">&#160;</td><td class="mdescRight">Stochastic variance reduced gradient with Barzilai-Borwein.  <a href="#ae0bcb29037b15bffb25fb75d31e2e4a5">More...</a><br /></br></td></tr>
<tr class="separator:ae0bcb29037b15bffb25fb75d31e2e4a5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a80af5f3b73ee02a159ca68ac20a50b51"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacemlpack_1_1optimization.html#a80af5f3b73ee02a159ca68ac20a50b51">StandardSGD</a> = <a class="el" href="classmlpack_1_1optimization_1_1SGD.html">SGD</a>&lt; <a class="el" href="classmlpack_1_1optimization_1_1VanillaUpdate.html">VanillaUpdate</a> &gt;</td></tr>
<tr class="separator:a80af5f3b73ee02a159ca68ac20a50b51"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8cf60766bdebcfd7ce8219c2a5ca8f36"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacemlpack_1_1optimization.html#a8cf60766bdebcfd7ce8219c2a5ca8f36">SVRG</a> = <a class="el" href="classmlpack_1_1optimization_1_1SVRGType.html">SVRGType</a>&lt; <a class="el" href="classmlpack_1_1optimization_1_1SVRGUpdate.html">SVRGUpdate</a>, <a class="el" href="classmlpack_1_1optimization_1_1NoDecay.html">NoDecay</a> &gt;</td></tr>
<tr class="memdesc:a8cf60766bdebcfd7ce8219c2a5ca8f36"><td class="mdescLeft">&#160;</td><td class="mdescRight">Standard stochastic variance reduced gradient.  <a href="#a8cf60766bdebcfd7ce8219c2a5ca8f36">More...</a><br /></br></td></tr>
<tr class="separator:a8cf60766bdebcfd7ce8219c2a5ca8f36"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa8806a595ca5c13a9dabc67c4ef86f24"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacemlpack_1_1optimization.html#aa8806a595ca5c13a9dabc67c4ef86f24">SVRG_BB</a> = <a class="el" href="classmlpack_1_1optimization_1_1SVRGType.html">SVRGType</a>&lt; <a class="el" href="classmlpack_1_1optimization_1_1SVRGUpdate.html">SVRGUpdate</a>, <a class="el" href="classmlpack_1_1optimization_1_1BarzilaiBorweinDecay.html">BarzilaiBorweinDecay</a> &gt;</td></tr>
<tr class="memdesc:aa8806a595ca5c13a9dabc67c4ef86f24"><td class="mdescLeft">&#160;</td><td class="mdescRight">Stochastic variance reduced gradient with Barzilai-Borwein.  <a href="#aa8806a595ca5c13a9dabc67c4ef86f24">More...</a><br /></br></td></tr>
<tr class="separator:aa8806a595ca5c13a9dabc67c4ef86f24"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<h2 class="groupheader">Typedef Documentation</h2>
<a id="ae015766100b47037db7b997462facc97"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae015766100b47037db7b997462facc97">&#9670;&nbsp;</a></span>Adam</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr >
          <td class="memname">using <a class="el" href="namespacemlpack_1_1optimization.html#ae015766100b47037db7b997462facc97">Adam</a> =  <a class="el" href="classmlpack_1_1optimization_1_1AdamType.html">AdamType</a>&lt;<a class="el" href="classmlpack_1_1optimization_1_1AdamUpdate.html">AdamUpdate</a>&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="adam_8hpp_source.html#l00180">180</a> of file <a class="el" href="adam_8hpp_source.html">adam.hpp</a>.</p>

</div>
</div>
<a id="a7d6274d40de7c01efc1ae34158a319cc"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7d6274d40de7c01efc1ae34158a319cc">&#9670;&nbsp;</a></span>AdaMax</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr >
          <td class="memname">using <a class="el" href="namespacemlpack_1_1optimization.html#a7d6274d40de7c01efc1ae34158a319cc">AdaMax</a> =  <a class="el" href="classmlpack_1_1optimization_1_1AdamType.html">AdamType</a>&lt;<a class="el" href="classmlpack_1_1optimization_1_1AdaMaxUpdate.html">AdaMaxUpdate</a>&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="adam_8hpp_source.html#l00182">182</a> of file <a class="el" href="adam_8hpp_source.html">adam.hpp</a>.</p>

</div>
</div>
<a id="a07b6ef20451d6c19952d44ff6c52e3bb"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a07b6ef20451d6c19952d44ff6c52e3bb">&#9670;&nbsp;</a></span>AMSGrad</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr >
          <td class="memname">using <a class="el" href="namespacemlpack_1_1optimization.html#a07b6ef20451d6c19952d44ff6c52e3bb">AMSGrad</a> =  <a class="el" href="classmlpack_1_1optimization_1_1AdamType.html">AdamType</a>&lt;<a class="el" href="classmlpack_1_1optimization_1_1AMSGradUpdate.html">AMSGradUpdate</a>&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="adam_8hpp_source.html#l00184">184</a> of file <a class="el" href="adam_8hpp_source.html">adam.hpp</a>.</p>

</div>
</div>
<a id="a4a29a6f28bda38f78a6068c898c7c20d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4a29a6f28bda38f78a6068c898c7c20d">&#9670;&nbsp;</a></span>ApproxCMAES</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr >
          <td class="memname">using <a class="el" href="namespacemlpack_1_1optimization.html#a4a29a6f28bda38f78a6068c898c7c20d">ApproxCMAES</a> =  <a class="el" href="classmlpack_1_1optimization_1_1CMAES.html">CMAES</a>&lt;SelectionPolicyType&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

<p >Convenient typedef for <a class="el" href="classmlpack_1_1optimization_1_1CMAES.html" title="CMA-ES - Covariance Matrix Adaptation Evolution Strategy is s a stochastic search algorithm...">CMAES</a> approximation. </p>

<p class="definition">Definition at line <a class="el" href="cmaes_8hpp_source.html#l00170">170</a> of file <a class="el" href="cmaes_8hpp_source.html">cmaes.hpp</a>.</p>

</div>
</div>
<a id="a78364270bd0338acc1a577f7f2ec369b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a78364270bd0338acc1a577f7f2ec369b">&#9670;&nbsp;</a></span>BBS_Armijo</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr >
          <td class="memname">using <a class="el" href="namespacemlpack_1_1optimization.html#a78364270bd0338acc1a577f7f2ec369b">BBS_Armijo</a> =  <a class="el" href="classmlpack_1_1optimization_1_1BigBatchSGD.html">BigBatchSGD</a>&lt;<a class="el" href="classmlpack_1_1optimization_1_1BacktrackingLineSearch.html">BacktrackingLineSearch</a>&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="bigbatch__sgd_8hpp_source.html#l00191">191</a> of file <a class="el" href="bigbatch__sgd_8hpp_source.html">bigbatch_sgd.hpp</a>.</p>

</div>
</div>
<a id="ad46e76af14b52ffbc541df79fe51a024"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad46e76af14b52ffbc541df79fe51a024">&#9670;&nbsp;</a></span>BBS_BB</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr >
          <td class="memname">using <a class="el" href="namespacemlpack_1_1optimization.html#ad46e76af14b52ffbc541df79fe51a024">BBS_BB</a> =  <a class="el" href="classmlpack_1_1optimization_1_1BigBatchSGD.html">BigBatchSGD</a>&lt;<a class="el" href="classmlpack_1_1optimization_1_1AdaptiveStepsize.html">AdaptiveStepsize</a>&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="bigbatch__sgd_8hpp_source.html#l00192">192</a> of file <a class="el" href="bigbatch__sgd_8hpp_source.html">bigbatch_sgd.hpp</a>.</p>

</div>
</div>
<a id="a06530afc92ee02d33b08e8c32860bee6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a06530afc92ee02d33b08e8c32860bee6">&#9670;&nbsp;</a></span>Katyusha</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr >
          <td class="memname">using <a class="el" href="namespacemlpack_1_1optimization.html#a06530afc92ee02d33b08e8c32860bee6">Katyusha</a> =  <a class="el" href="classmlpack_1_1optimization_1_1KatyushaType.html">KatyushaType</a>&lt;false&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

<p >Katyusha using the standard update step. </p>

<p class="definition">Definition at line <a class="el" href="katyusha_8hpp_source.html#l00171">171</a> of file <a class="el" href="katyusha_8hpp_source.html">katyusha.hpp</a>.</p>

</div>
</div>
<a id="a6654a0baa3bab4b02d9b6e5eb901397f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6654a0baa3bab4b02d9b6e5eb901397f">&#9670;&nbsp;</a></span>KatyushaProximal</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr >
          <td class="memname">using <a class="el" href="namespacemlpack_1_1optimization.html#a6654a0baa3bab4b02d9b6e5eb901397f">KatyushaProximal</a> =  <a class="el" href="classmlpack_1_1optimization_1_1KatyushaType.html">KatyushaType</a>&lt;true&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

<p >Katyusha using the proximal update step. </p>

<p class="definition">Definition at line <a class="el" href="katyusha_8hpp_source.html#l00176">176</a> of file <a class="el" href="katyusha_8hpp_source.html">katyusha.hpp</a>.</p>

</div>
</div>
<a id="ae5ae5626c8ca6228b90f6e11c02eb1f8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae5ae5626c8ca6228b90f6e11c02eb1f8">&#9670;&nbsp;</a></span>MomentumSGD</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr >
          <td class="memname">using <a class="el" href="namespacemlpack_1_1optimization.html#ae5ae5626c8ca6228b90f6e11c02eb1f8">MomentumSGD</a> =  <a class="el" href="classmlpack_1_1optimization_1_1SGD.html">SGD</a>&lt;MomentumUpdate&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="sgd_8hpp_source.html#l00205">205</a> of file <a class="el" href="sgd_8hpp_source.html">sgd.hpp</a>.</p>

</div>
</div>
<a id="a373a947f96aee47a23b0f24a9760c2a4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a373a947f96aee47a23b0f24a9760c2a4">&#9670;&nbsp;</a></span>Nadam</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr >
          <td class="memname">using <a class="el" href="namespacemlpack_1_1optimization.html#a373a947f96aee47a23b0f24a9760c2a4">Nadam</a> =  <a class="el" href="classmlpack_1_1optimization_1_1AdamType.html">AdamType</a>&lt;<a class="el" href="classmlpack_1_1optimization_1_1NadamUpdate.html">NadamUpdate</a>&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="adam_8hpp_source.html#l00186">186</a> of file <a class="el" href="adam_8hpp_source.html">adam.hpp</a>.</p>

</div>
</div>
<a id="ad8acb27595de9dc717961e6e9b0d89bd"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad8acb27595de9dc717961e6e9b0d89bd">&#9670;&nbsp;</a></span>NadaMax</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr >
          <td class="memname">using <a class="el" href="namespacemlpack_1_1optimization.html#ad8acb27595de9dc717961e6e9b0d89bd">NadaMax</a> =  <a class="el" href="classmlpack_1_1optimization_1_1AdamType.html">AdamType</a>&lt;<a class="el" href="classmlpack_1_1optimization_1_1NadaMaxUpdate.html">NadaMaxUpdate</a>&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="adam_8hpp_source.html#l00188">188</a> of file <a class="el" href="adam_8hpp_source.html">adam.hpp</a>.</p>

</div>
</div>
<a id="a92d67fc88582396a07a0ed76e72329c6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a92d67fc88582396a07a0ed76e72329c6">&#9670;&nbsp;</a></span>NesterovMomentumSGD</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr >
          <td class="memname">using <a class="el" href="namespacemlpack_1_1optimization.html#a92d67fc88582396a07a0ed76e72329c6">NesterovMomentumSGD</a> =  <a class="el" href="classmlpack_1_1optimization_1_1SGD.html">SGD</a>&lt;<a class="el" href="classmlpack_1_1optimization_1_1NesterovMomentumUpdate.html">NesterovMomentumUpdate</a>&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="sgd_8hpp_source.html#l00207">207</a> of file <a class="el" href="sgd_8hpp_source.html">sgd.hpp</a>.</p>

</div>
</div>
<a id="a54120a755fc1e801162e212657f6ceff"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a54120a755fc1e801162e212657f6ceff">&#9670;&nbsp;</a></span>OMP</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr >
          <td class="memname">using <a class="el" href="namespacemlpack_1_1optimization.html#a54120a755fc1e801162e212657f6ceff">OMP</a> =  <a class="el" href="classmlpack_1_1optimization_1_1FrankWolfe.html">FrankWolfe</a>&lt;<a class="el" href="classmlpack_1_1optimization_1_1ConstrLpBallSolver.html">ConstrLpBallSolver</a>, <a class="el" href="classmlpack_1_1optimization_1_1UpdateSpan.html">UpdateSpan</a>&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

<p >Orthogonal Matching Pursuit. </p>
<p >It is a sparse approximation algorithm which involves finding the "best matching" projections of multidimensional data onto the span of an over-complete dictionary. To use it, the dictionary is input as the columns of MatrixA() in <a class="el" href="classmlpack_1_1optimization_1_1FuncSq.html" title="Square loss function . ">FuncSq</a> class, and the vector to be approximated is input as the Vectorb() in <a class="el" href="classmlpack_1_1optimization_1_1FuncSq.html" title="Square loss function . ">FuncSq</a> class. </p>

<p class="definition">Definition at line <a class="el" href="frank__wolfe_8hpp_source.html#l00162">162</a> of file <a class="el" href="frank__wolfe_8hpp_source.html">frank_wolfe.hpp</a>.</p>

</div>
</div>
<a id="aae31eb5e7dce7f22edfe89018785e632"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aae31eb5e7dce7f22edfe89018785e632">&#9670;&nbsp;</a></span>OptimisticAdam</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr >
          <td class="memname">using <a class="el" href="namespacemlpack_1_1optimization.html#aae31eb5e7dce7f22edfe89018785e632">OptimisticAdam</a> =  <a class="el" href="classmlpack_1_1optimization_1_1AdamType.html">AdamType</a>&lt;<a class="el" href="classmlpack_1_1optimization_1_1OptimisticAdamUpdate.html">OptimisticAdamUpdate</a>&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="adam_8hpp_source.html#l00190">190</a> of file <a class="el" href="adam_8hpp_source.html">adam.hpp</a>.</p>

</div>
</div>
<a id="af3f51b1e39d7b8c88aef4c0a3cd31ddd"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af3f51b1e39d7b8c88aef4c0a3cd31ddd">&#9670;&nbsp;</a></span>SARAH</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr >
          <td class="memname">using <a class="el" href="namespacemlpack_1_1optimization.html#af3f51b1e39d7b8c88aef4c0a3cd31ddd">SARAH</a> =  <a class="el" href="classmlpack_1_1optimization_1_1SARAHType.html">SARAHType</a>&lt;<a class="el" href="classmlpack_1_1optimization_1_1SARAHUpdate.html">SARAHUpdate</a>&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

<p >Standard stochastic variance reduced gradient. </p>

<p class="definition">Definition at line <a class="el" href="sarah_8hpp_source.html#l00175">175</a> of file <a class="el" href="sarah_8hpp_source.html">sarah.hpp</a>.</p>

</div>
</div>
<a id="ae0bcb29037b15bffb25fb75d31e2e4a5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae0bcb29037b15bffb25fb75d31e2e4a5">&#9670;&nbsp;</a></span>SARAH_Plus</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr >
          <td class="memname">using <a class="el" href="namespacemlpack_1_1optimization.html#ae0bcb29037b15bffb25fb75d31e2e4a5">SARAH_Plus</a> =  <a class="el" href="classmlpack_1_1optimization_1_1SARAHType.html">SARAHType</a>&lt;<a class="el" href="classmlpack_1_1optimization_1_1SARAHPlusUpdate.html">SARAHPlusUpdate</a>&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

<p >Stochastic variance reduced gradient with Barzilai-Borwein. </p>

<p class="definition">Definition at line <a class="el" href="sarah_8hpp_source.html#l00180">180</a> of file <a class="el" href="sarah_8hpp_source.html">sarah.hpp</a>.</p>

</div>
</div>
<a id="a80af5f3b73ee02a159ca68ac20a50b51"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a80af5f3b73ee02a159ca68ac20a50b51">&#9670;&nbsp;</a></span>StandardSGD</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr >
          <td class="memname">using <a class="el" href="namespacemlpack_1_1optimization.html#a80af5f3b73ee02a159ca68ac20a50b51">StandardSGD</a> =  <a class="el" href="classmlpack_1_1optimization_1_1SGD.html">SGD</a>&lt;<a class="el" href="classmlpack_1_1optimization_1_1VanillaUpdate.html">VanillaUpdate</a>&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="sgd_8hpp_source.html#l00203">203</a> of file <a class="el" href="sgd_8hpp_source.html">sgd.hpp</a>.</p>

</div>
</div>
<a id="a8cf60766bdebcfd7ce8219c2a5ca8f36"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8cf60766bdebcfd7ce8219c2a5ca8f36">&#9670;&nbsp;</a></span>SVRG</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr >
          <td class="memname">using <a class="el" href="namespacemlpack_1_1optimization.html#a8cf60766bdebcfd7ce8219c2a5ca8f36">SVRG</a> =  <a class="el" href="classmlpack_1_1optimization_1_1SVRGType.html">SVRGType</a>&lt;<a class="el" href="classmlpack_1_1optimization_1_1SVRGUpdate.html">SVRGUpdate</a>, <a class="el" href="classmlpack_1_1optimization_1_1NoDecay.html">NoDecay</a>&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

<p >Standard stochastic variance reduced gradient. </p>

<p class="definition">Definition at line <a class="el" href="svrg_8hpp_source.html#l00234">234</a> of file <a class="el" href="svrg_8hpp_source.html">svrg.hpp</a>.</p>

</div>
</div>
<a id="aa8806a595ca5c13a9dabc67c4ef86f24"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa8806a595ca5c13a9dabc67c4ef86f24">&#9670;&nbsp;</a></span>SVRG_BB</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr >
          <td class="memname">using <a class="el" href="namespacemlpack_1_1optimization.html#aa8806a595ca5c13a9dabc67c4ef86f24">SVRG_BB</a> =  <a class="el" href="classmlpack_1_1optimization_1_1SVRGType.html">SVRGType</a>&lt;<a class="el" href="classmlpack_1_1optimization_1_1SVRGUpdate.html">SVRGUpdate</a>, <a class="el" href="classmlpack_1_1optimization_1_1BarzilaiBorweinDecay.html">BarzilaiBorweinDecay</a>&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

<p >Stochastic variance reduced gradient with Barzilai-Borwein. </p>

<p class="definition">Definition at line <a class="el" href="svrg_8hpp_source.html#l00239">239</a> of file <a class="el" href="svrg_8hpp_source.html">svrg.hpp</a>.</p>

</div>
</div>
</div>

<hr class="footer"></hr><address class="footer"><small >
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"></img>
</a> 1.8.13
</small></address>
</div>
</body>
<script type="text/javascript">
var x = document.querySelectorAll("img.formulaDsp");
var i;
for (i = 0; i < x.length; i++)
{
  x[i].width = x[i].offsetWidth / 4;
}
</script>
</html>