<html >
<head >

<meta name="keywords" content="mlpack, libmlpack, c++, armadillo, machine
learning, data mining, classification, regression, tree-based methods, dual-tree
algorithm">
<meta name="description" content="mlpack: a scalable c++ machine learning
library">
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title >mlpack: a scalable c++ machine learning library</title>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript" src="dynamic_tables.js"></script>
</head><link rel="stylesheet" href="style-doxygen.css" /><link rel="stylesheet" href="doxygen.css" /><link rel="stylesheet" href="tabs.css" /><link rel="stylesheet" href="search/search.css" /><link href="http://fonts.googleapis.com/css?family=Maven+Pro:500" rel="stylesheet" type="text/css" />





<body ><br />


<div class="mlpack_titlebar">
   <a href="http://www.mlpack.org"><img src="../../../mlpack.png"></a>
</div>
<center >
<div class="mlnavbar">
  <div class="navcontainer">
   <div class="mlnavitem" name="mlnavmain"><a href="../../../index.html">main</a></div>
   <div class="mlnavitem" name="mlnavabout"><a href="../../../about.html">about</a></div>
   <div class="mlnavitem" name="mlnavdoc"><a href="../../../docs.html">docs</a></div>
   <div class="mlnavitem" name="mlnavhelp"><a href="../../../help.html">get help</a></div>
   <div class="mlnavitem" name="mlnavbugs"><a href="https://github.com/mlpack/mlpack">github</a></div>
  </div>
</div>
</center>
<div class="separator"></div>
<center >
<div class="mainsection smallertext">
<div id="top">
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody >
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">mlpack
   &#160;<span id="projectnumber">master</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>

<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>

<div id="MSearchSelectWindow" onmouseover="return searchBox.OnSearchSelectShow()" onmouseout="return searchBox.OnSearchSelectHide()" onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>


<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul >
<li class="navelem"><a class="el" href="namespacemlpack.html">mlpack</a></li><li class="navelem"><a class="el" href="namespacemlpack_1_1optimization.html">optimization</a></li>  </ul>
</div>
</div>
<div class="header">
  <div class="summary">
<a href="#namespaces">Namespaces</a> &#124;
<a href="#nested-classes">Classes</a> &#124;
<a href="#typedef-members">Typedefs</a>  </div>
  <div class="headertitle">
<div class="title">mlpack::optimization Namespace Reference</div>  </div>
</div>
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="namespaces"></a>
Namespaces</h2></td></tr>
<tr class="memitem:namespacemlpack_1_1optimization_1_1test"><td class="memItemLeft" align="right" valign="top"> &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacemlpack_1_1optimization_1_1test.html">test</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="nested-classes"></a>
Classes</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AdaDelta.html">AdaDelta</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight"><a class="el" href="classmlpack_1_1optimization_1_1AdaDelta.html" title="AdaDelta is an optimizer that uses two ideas to improve upon the two main drawbacks of the Adagrad me...">AdaDelta</a> is an optimizer that uses two ideas to improve upon the two main drawbacks of the Adagrad method:  <a href="classmlpack_1_1optimization_1_1AdaDelta.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AdaDeltaUpdate.html">AdaDeltaUpdate</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Implementation of the <a class="el" href="classmlpack_1_1optimization_1_1AdaDelta.html" title="AdaDelta is an optimizer that uses two ideas to improve upon the two main drawbacks of the Adagrad me...">AdaDelta</a> update policy.  <a href="classmlpack_1_1optimization_1_1AdaDeltaUpdate.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AdaGrad.html">AdaGrad</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight"><a class="el" href="classmlpack_1_1optimization_1_1AdaGrad.html" title="AdaGrad is a modified version of stochastic gradient descent which performs larger updates for more s...">AdaGrad</a> is a modified version of stochastic gradient descent which performs larger updates for more sparse parameters and smaller updates for less sparse parameters.  <a href="classmlpack_1_1optimization_1_1AdaGrad.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AdaGradUpdate.html">AdaGradUpdate</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Implementation of the <a class="el" href="classmlpack_1_1optimization_1_1AdaGrad.html" title="AdaGrad is a modified version of stochastic gradient descent which performs larger updates for more s...">AdaGrad</a> update policy.  <a href="classmlpack_1_1optimization_1_1AdaGradUpdate.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AdaMaxUpdate.html">AdaMaxUpdate</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">AdaMax is a variant of Adam, an optimizer that computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients.based on the infinity norm as given in the section 7 of the following paper.  <a href="classmlpack_1_1optimization_1_1AdaMaxUpdate.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AdamType.html">AdamType</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Adam is an optimizer that computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients.  <a href="classmlpack_1_1optimization_1_1AdamType.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AdamUpdate.html">AdamUpdate</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Adam is an optimizer that computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients as given in the section 7 of the following paper.  <a href="classmlpack_1_1optimization_1_1AdamUpdate.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AugLagrangian.html">AugLagrangian</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">The <a class="el" href="classmlpack_1_1optimization_1_1AugLagrangian.html" title="The AugLagrangian class implements the Augmented Lagrangian method of optimization. ">AugLagrangian</a> class implements the Augmented Lagrangian method of optimization.  <a href="classmlpack_1_1optimization_1_1AugLagrangian.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AugLagrangianFunction.html">AugLagrangianFunction</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">This is a utility class used by <a class="el" href="classmlpack_1_1optimization_1_1AugLagrangian.html" title="The AugLagrangian class implements the Augmented Lagrangian method of optimization. ">AugLagrangian</a>, meant to wrap a LagrangianFunction into a function usable by a simple optimizer like L-BFGS.  <a href="classmlpack_1_1optimization_1_1AugLagrangianFunction.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1AugLagrangianTestFunction.html">AugLagrangianTestFunction</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">This function is taken from "Practical Mathematical Optimization" (Snyman), section 5.3.8 ("Application of the Augmented Lagrangian Method").  <a href="classmlpack_1_1optimization_1_1AugLagrangianTestFunction.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1CNE.html">CNE</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Conventional Neural Evolution (<a class="el" href="classmlpack_1_1optimization_1_1CNE.html" title="Conventional Neural Evolution (CNE) is a class of evolutionary algorithms focused on dealing with fix...">CNE</a>) is a class of evolutionary algorithms focused on dealing with fixed topology.  <a href="classmlpack_1_1optimization_1_1CNE.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1ConstantStep.html">ConstantStep</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Implementation of the <a class="el" href="classmlpack_1_1optimization_1_1ConstantStep.html" title="Implementation of the ConstantStep stepsize decay policy for parallel SGD. ">ConstantStep</a> stepsize decay policy for parallel <a class="el" href="classmlpack_1_1optimization_1_1SGD.html" title="Stochastic Gradient Descent is a technique for minimizing a function which can be expressed as a sum ...">SGD</a>.  <a href="classmlpack_1_1optimization_1_1ConstantStep.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1ConstrLpBallSolver.html">ConstrLpBallSolver</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">LinearConstrSolver for <a class="el" href="classmlpack_1_1optimization_1_1FrankWolfe.html" title="Frank-Wolfe is a technique to minimize a continuously differentiable convex function  over a compact ...">FrankWolfe</a> algorithm.  <a href="classmlpack_1_1optimization_1_1ConstrLpBallSolver.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1CyclicalDecay.html">CyclicalDecay</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Simulate a new warm-started run/restart once a number of epochs are performed.  <a href="classmlpack_1_1optimization_1_1CyclicalDecay.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1ExponentialBackoff.html">ExponentialBackoff</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Exponential backoff stepsize reduction policy for parallel <a class="el" href="classmlpack_1_1optimization_1_1SGD.html" title="Stochastic Gradient Descent is a technique for minimizing a function which can be expressed as a sum ...">SGD</a>.  <a href="classmlpack_1_1optimization_1_1ExponentialBackoff.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1ExponentialSchedule.html">ExponentialSchedule</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">The exponential cooling schedule cools the temperature T at every step according to the equation.  <a href="classmlpack_1_1optimization_1_1ExponentialSchedule.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1FrankWolfe.html">FrankWolfe</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Frank-Wolfe is a technique to minimize a continuously differentiable convex function <img class="formulaInl" alt="$ f $" src="form_64.png"></img> over a compact convex subset <img class="formulaInl" alt="$ D $" src="form_54.png"></img> of a vector space.  <a href="classmlpack_1_1optimization_1_1FrankWolfe.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1FuncSq.html">FuncSq</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Square loss function <img class="formulaInl" alt="$ f(x) = 0.5 * ||Ax - b||_2^2 $" src="form_75.png"></img>.  <a href="classmlpack_1_1optimization_1_1FuncSq.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1GockenbachFunction.html">GockenbachFunction</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">This function is taken from M.  <a href="classmlpack_1_1optimization_1_1GockenbachFunction.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1GradientClipping.html">GradientClipping</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Interface for wrapping around update policies (e.g., <a class="el" href="classmlpack_1_1optimization_1_1VanillaUpdate.html" title="Vanilla update policy for Stochastic Gradient Descent (SGD). ">VanillaUpdate</a>) and feeding a clipped gradient to them instead of the normal one.  <a href="classmlpack_1_1optimization_1_1GradientClipping.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1GradientDescent.html">GradientDescent</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Gradient Descent is a technique to minimize a function.  <a href="classmlpack_1_1optimization_1_1GradientDescent.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1L__BFGS.html">L_BFGS</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">The generic L-BFGS optimizer, which uses a back-tracking line search algorithm to minimize a function.  <a href="classmlpack_1_1optimization_1_1L__BFGS.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1LovaszThetaSDP.html">LovaszThetaSDP</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">This function is the Lovasz-Theta semidefinite program, as implemented in the following paper:  <a href="classmlpack_1_1optimization_1_1LovaszThetaSDP.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1LRSDP.html">LRSDP</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight"><a class="el" href="classmlpack_1_1optimization_1_1LRSDP.html" title="LRSDP is the implementation of Monteiro and Burer&apos;s formulation of low-rank semidefinite programs (LR...">LRSDP</a> is the implementation of Monteiro and Burer's formulation of low-rank semidefinite programs (LR-SDP).  <a href="classmlpack_1_1optimization_1_1LRSDP.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1LRSDPFunction.html">LRSDPFunction</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">The objective function that <a class="el" href="classmlpack_1_1optimization_1_1LRSDP.html" title="LRSDP is the implementation of Monteiro and Burer&apos;s formulation of low-rank semidefinite programs (LR...">LRSDP</a> is trying to optimize.  <a href="classmlpack_1_1optimization_1_1LRSDPFunction.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1MiniBatchSGDType.html">MiniBatchSGDType</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Mini-batch Stochastic Gradient Descent is a technique for minimizing a function which can be expressed as a sum of other functions.  <a href="classmlpack_1_1optimization_1_1MiniBatchSGDType.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1NoDecay.html">NoDecay</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Definition of the <a class="el" href="classmlpack_1_1optimization_1_1NoDecay.html" title="Definition of the NoDecay class. ">NoDecay</a> class.  <a href="classmlpack_1_1optimization_1_1NoDecay.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1ParallelSGD.html">ParallelSGD</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">An implementation of parallel stochastic gradient descent using the lock-free HOGWILD! approach.  <a href="classmlpack_1_1optimization_1_1ParallelSGD.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1PrimalDualSolver.html">PrimalDualSolver</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Interface to a primal dual interior point solver.  <a href="classmlpack_1_1optimization_1_1PrimalDualSolver.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1RMSProp.html">RMSProp</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight"><a class="el" href="classmlpack_1_1optimization_1_1RMSProp.html" title="RMSProp is an optimizer that utilizes the magnitude of recent gradients to normalize the gradients...">RMSProp</a> is an optimizer that utilizes the magnitude of recent gradients to normalize the gradients.  <a href="classmlpack_1_1optimization_1_1RMSProp.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1RMSPropUpdate.html">RMSPropUpdate</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight"><a class="el" href="classmlpack_1_1optimization_1_1RMSProp.html" title="RMSProp is an optimizer that utilizes the magnitude of recent gradients to normalize the gradients...">RMSProp</a> is an optimizer that utilizes the magnitude of recent gradients to normalize the gradients.  <a href="classmlpack_1_1optimization_1_1RMSPropUpdate.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1SA.html">SA</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Simulated Annealing is an stochastic optimization algorithm which is able to deliver near-optimal results quickly without knowing the gradient of the function being optimized.  <a href="classmlpack_1_1optimization_1_1SA.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1SDP.html">SDP</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Specify an <a class="el" href="classmlpack_1_1optimization_1_1SDP.html" title="Specify an SDP in primal form. ">SDP</a> in primal form.  <a href="classmlpack_1_1optimization_1_1SDP.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1SGD.html">SGD</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Stochastic Gradient Descent is a technique for minimizing a function which can be expressed as a sum of other functions.  <a href="classmlpack_1_1optimization_1_1SGD.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1SGDR.html">SGDR</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">This class is based on Mini-batch Stochastic Gradient Descent class and simulates a new warm-started run/restart once a number of epochs are performed.  <a href="classmlpack_1_1optimization_1_1SGDR.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1SMORMS3.html">SMORMS3</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight"><a class="el" href="classmlpack_1_1optimization_1_1SMORMS3.html" title="SMORMS3 is an optimizer that estimates a safe and optimal distance based on curvature and normalizing...">SMORMS3</a> is an optimizer that estimates a safe and optimal distance based on curvature and normalizing the stepsize in the parameter space.  <a href="classmlpack_1_1optimization_1_1SMORMS3.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1SMORMS3Update.html">SMORMS3Update</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight"><a class="el" href="classmlpack_1_1optimization_1_1SMORMS3.html" title="SMORMS3 is an optimizer that estimates a safe and optimal distance based on curvature and normalizing...">SMORMS3</a> is an optimizer that estimates a safe and optimal distance based on curvature and normalizing the stepsize in the parameter space.  <a href="classmlpack_1_1optimization_1_1SMORMS3Update.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1SnapshotEnsembles.html">SnapshotEnsembles</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Simulate a new warm-started run/restart once a number of epochs are performed.  <a href="classmlpack_1_1optimization_1_1SnapshotEnsembles.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1SnapshotSGDR.html">SnapshotSGDR</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">This class is based on Mini-batch Stochastic Gradient Descent class and simulates a new warm-started run/restart once a number of epochs are performed using the Snapshot ensembles technique.  <a href="classmlpack_1_1optimization_1_1SnapshotSGDR.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1TestFuncFW.html">TestFuncFW</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Simple test function for classic Frank Wolfe Algorithm:  <a href="classmlpack_1_1optimization_1_1TestFuncFW.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1UpdateClassic.html">UpdateClassic</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Use classic rule in the update step for <a class="el" href="classmlpack_1_1optimization_1_1FrankWolfe.html" title="Frank-Wolfe is a technique to minimize a continuously differentiable convex function  over a compact ...">FrankWolfe</a> algorithm.  <a href="classmlpack_1_1optimization_1_1UpdateClassic.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1UpdateSpan.html">UpdateSpan</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Recalculate the optimal solution in the span of all previous solution space, used as update step for <a class="el" href="classmlpack_1_1optimization_1_1FrankWolfe.html" title="Frank-Wolfe is a technique to minimize a continuously differentiable convex function  over a compact ...">FrankWolfe</a> algorithm.  <a href="classmlpack_1_1optimization_1_1UpdateSpan.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classmlpack_1_1optimization_1_1VanillaUpdate.html">VanillaUpdate</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Vanilla update policy for Stochastic Gradient Descent (<a class="el" href="classmlpack_1_1optimization_1_1SGD.html" title="Stochastic Gradient Descent is a technique for minimizing a function which can be expressed as a sum ...">SGD</a>).  <a href="classmlpack_1_1optimization_1_1VanillaUpdate.html#details">More...</a><br /></br></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="typedef-members"></a>
Typedefs</h2></td></tr>
<tr class="memitem:ae015766100b47037db7b997462facc97"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacemlpack_1_1optimization.html#ae015766100b47037db7b997462facc97">Adam</a> = <a class="el" href="classmlpack_1_1optimization_1_1AdamType.html">AdamType</a>&lt; <a class="el" href="classmlpack_1_1optimization_1_1AdamUpdate.html">AdamUpdate</a> &gt;</td></tr>
<tr class="separator:ae015766100b47037db7b997462facc97"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7d6274d40de7c01efc1ae34158a319cc"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacemlpack_1_1optimization.html#a7d6274d40de7c01efc1ae34158a319cc">AdaMax</a> = <a class="el" href="classmlpack_1_1optimization_1_1AdamType.html">AdamType</a>&lt; <a class="el" href="classmlpack_1_1optimization_1_1AdaMaxUpdate.html">AdaMaxUpdate</a> &gt;</td></tr>
<tr class="separator:a7d6274d40de7c01efc1ae34158a319cc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad34667a7e3fe83dc74d28775e5e6720c"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacemlpack_1_1optimization.html#ad34667a7e3fe83dc74d28775e5e6720c">MiniBatchSGD</a> = <a class="el" href="classmlpack_1_1optimization_1_1MiniBatchSGDType.html">MiniBatchSGDType</a>&lt; <a class="el" href="classmlpack_1_1optimization_1_1VanillaUpdate.html">VanillaUpdate</a>, <a class="el" href="classmlpack_1_1optimization_1_1NoDecay.html">NoDecay</a> &gt;</td></tr>
<tr class="separator:ad34667a7e3fe83dc74d28775e5e6720c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae5ae5626c8ca6228b90f6e11c02eb1f8"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacemlpack_1_1optimization.html#ae5ae5626c8ca6228b90f6e11c02eb1f8">MomentumSGD</a> = <a class="el" href="classmlpack_1_1optimization_1_1SGD.html">SGD</a>&lt; MomentumUpdate &gt;</td></tr>
<tr class="separator:ae5ae5626c8ca6228b90f6e11c02eb1f8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a54120a755fc1e801162e212657f6ceff"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacemlpack_1_1optimization.html#a54120a755fc1e801162e212657f6ceff">OMP</a> = <a class="el" href="classmlpack_1_1optimization_1_1FrankWolfe.html">FrankWolfe</a>&lt; <a class="el" href="classmlpack_1_1optimization_1_1ConstrLpBallSolver.html">ConstrLpBallSolver</a>, <a class="el" href="classmlpack_1_1optimization_1_1UpdateSpan.html">UpdateSpan</a> &gt;</td></tr>
<tr class="memdesc:a54120a755fc1e801162e212657f6ceff"><td class="mdescLeft">&#160;</td><td class="mdescRight">Orthogonal Matching Pursuit.  <a href="#a54120a755fc1e801162e212657f6ceff">More...</a><br /></br></td></tr>
<tr class="separator:a54120a755fc1e801162e212657f6ceff"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a80af5f3b73ee02a159ca68ac20a50b51"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacemlpack_1_1optimization.html#a80af5f3b73ee02a159ca68ac20a50b51">StandardSGD</a> = <a class="el" href="classmlpack_1_1optimization_1_1SGD.html">SGD</a>&lt; <a class="el" href="classmlpack_1_1optimization_1_1VanillaUpdate.html">VanillaUpdate</a> &gt;</td></tr>
<tr class="separator:a80af5f3b73ee02a159ca68ac20a50b51"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<h2 class="groupheader">Typedef Documentation</h2>
<a id="ae015766100b47037db7b997462facc97"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae015766100b47037db7b997462facc97">&#9670;&nbsp;</a></span>Adam</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr >
          <td class="memname">using <a class="el" href="namespacemlpack_1_1optimization.html#ae015766100b47037db7b997462facc97">Adam</a> =  <a class="el" href="classmlpack_1_1optimization_1_1AdamType.html">AdamType</a>&lt;<a class="el" href="classmlpack_1_1optimization_1_1AdamUpdate.html">AdamUpdate</a>&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="adam_8hpp_source.html#l00155">155</a> of file <a class="el" href="adam_8hpp_source.html">adam.hpp</a>.</p>

</div>
</div>
<a id="a7d6274d40de7c01efc1ae34158a319cc"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7d6274d40de7c01efc1ae34158a319cc">&#9670;&nbsp;</a></span>AdaMax</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr >
          <td class="memname">using <a class="el" href="namespacemlpack_1_1optimization.html#a7d6274d40de7c01efc1ae34158a319cc">AdaMax</a> =  <a class="el" href="classmlpack_1_1optimization_1_1AdamType.html">AdamType</a>&lt;<a class="el" href="classmlpack_1_1optimization_1_1AdaMaxUpdate.html">AdaMaxUpdate</a>&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="adam_8hpp_source.html#l00157">157</a> of file <a class="el" href="adam_8hpp_source.html">adam.hpp</a>.</p>

</div>
</div>
<a id="ad34667a7e3fe83dc74d28775e5e6720c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad34667a7e3fe83dc74d28775e5e6720c">&#9670;&nbsp;</a></span>MiniBatchSGD</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr >
          <td class="memname">using <a class="el" href="namespacemlpack_1_1optimization.html#ad34667a7e3fe83dc74d28775e5e6720c">MiniBatchSGD</a> =  <a class="el" href="classmlpack_1_1optimization_1_1MiniBatchSGDType.html">MiniBatchSGDType</a>&lt;<a class="el" href="classmlpack_1_1optimization_1_1VanillaUpdate.html">VanillaUpdate</a>, <a class="el" href="classmlpack_1_1optimization_1_1NoDecay.html">NoDecay</a>&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="minibatch__sgd_8hpp_source.html#l00206">206</a> of file <a class="el" href="minibatch__sgd_8hpp_source.html">minibatch_sgd.hpp</a>.</p>

</div>
</div>
<a id="ae5ae5626c8ca6228b90f6e11c02eb1f8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae5ae5626c8ca6228b90f6e11c02eb1f8">&#9670;&nbsp;</a></span>MomentumSGD</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr >
          <td class="memname">using <a class="el" href="namespacemlpack_1_1optimization.html#ae5ae5626c8ca6228b90f6e11c02eb1f8">MomentumSGD</a> =  <a class="el" href="classmlpack_1_1optimization_1_1SGD.html">SGD</a>&lt;MomentumUpdate&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="sgd_8hpp_source.html#l00177">177</a> of file <a class="el" href="sgd_8hpp_source.html">sgd.hpp</a>.</p>

</div>
</div>
<a id="a54120a755fc1e801162e212657f6ceff"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a54120a755fc1e801162e212657f6ceff">&#9670;&nbsp;</a></span>OMP</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr >
          <td class="memname">using <a class="el" href="namespacemlpack_1_1optimization.html#a54120a755fc1e801162e212657f6ceff">OMP</a> =  <a class="el" href="classmlpack_1_1optimization_1_1FrankWolfe.html">FrankWolfe</a>&lt;<a class="el" href="classmlpack_1_1optimization_1_1ConstrLpBallSolver.html">ConstrLpBallSolver</a>, <a class="el" href="classmlpack_1_1optimization_1_1UpdateSpan.html">UpdateSpan</a>&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

<p >Orthogonal Matching Pursuit. </p>
<p >It is a sparse approximation algorithm which involves finding the "best matching" projections of multidimensional data onto the span of an over-complete dictionary. To use it, the dictionary is input as the columns of MatrixA() in <a class="el" href="classmlpack_1_1optimization_1_1FuncSq.html" title="Square loss function . ">FuncSq</a> class, and the vector to be approximated is input as the Vectorb() in <a class="el" href="classmlpack_1_1optimization_1_1FuncSq.html" title="Square loss function . ">FuncSq</a> class. </p>

<p class="definition">Definition at line <a class="el" href="frank__wolfe_8hpp_source.html#l00165">165</a> of file <a class="el" href="frank__wolfe_8hpp_source.html">frank_wolfe.hpp</a>.</p>

</div>
</div>
<a id="a80af5f3b73ee02a159ca68ac20a50b51"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a80af5f3b73ee02a159ca68ac20a50b51">&#9670;&nbsp;</a></span>StandardSGD</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr >
          <td class="memname">using <a class="el" href="namespacemlpack_1_1optimization.html#a80af5f3b73ee02a159ca68ac20a50b51">StandardSGD</a> =  <a class="el" href="classmlpack_1_1optimization_1_1SGD.html">SGD</a>&lt;<a class="el" href="classmlpack_1_1optimization_1_1VanillaUpdate.html">VanillaUpdate</a>&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="sgd_8hpp_source.html#l00175">175</a> of file <a class="el" href="sgd_8hpp_source.html">sgd.hpp</a>.</p>

</div>
</div>
</div>

<hr class="footer"></hr><address class="footer"><small >
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"></img>
</a> 1.8.13
</small></address>
</div>
</body>
<script type="text/javascript">
var x = document.querySelectorAll("img.formulaDsp");
var i;
for (i = 0; i < x.length; i++)
{
  x[i].width = x[i].offsetWidth / 4;
}
</script>
</html>